{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e382b0b",
   "metadata": {},
   "source": [
    "# Importing modules and lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a636d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random as random\n",
    "import re\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.signal import savgol_filter\n",
    "import scipy.spatial.distance as ssd\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from MY_FUNCTIONS import *\n",
    "import seaborn as sns\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "# Load local paths and variables\n",
    "data = None\n",
    "with open(\"config.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "    \n",
    "input_path = join(data[\"general\"][\"input_path\"], \"speech_rate\")\n",
    "output_path = join(data[\"general\"][\"output_path\"], \"speech_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18e2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv(join(input_path, 'Lexique383\\\\Lexique383.tsv'), sep='\\t')\n",
    "lexicon = lexicon[['ortho', 'nbsyll']]\n",
    "dictionnary = dict(zip(lexicon['ortho'], lexicon ['nbsyll']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f4cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = join(output_path, \"plots\")\n",
    "examples_path = join(output_path, \"examples\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cf160",
   "metadata": {},
   "source": [
    "# Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad0831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_silences = 0  #silences between words of a same utterance\n",
    "remove_shorter = 1  #utterances with only one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964d3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_red = (0.8,0,0)\n",
    "my_blue= (0,0.4,0.8)\n",
    "my_pink = (1,0.4,0.4)\n",
    "my_green = (0,0.8,0.4)\n",
    "my_purple = (0.6,0,0.3)\n",
    "my_yellow = (0.89, 0.75, 0)\n",
    "my_orange = (0.82, 0.55, 0)\n",
    "my_turquoise = (0, 0.72, 0.58)\n",
    "my_bordeaux = (0.51, 0.02, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4565a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colors = [my_red, my_blue, my_pink, my_green, my_purple, my_yellow, my_orange, my_turquoise, my_bordeaux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac6fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_window = 9\n",
    "default_poly = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_AA = join(input_path, 'Corrected transcriptions\\\\AA')\n",
    "path_CA = join(input_path, 'Corrected transcriptions\\\\CA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c46181",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35404661",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = pd.read_csv(join(input_path, \"word.csv\"))\n",
    "WORDS = WORDS[WORDS['Length']!=0] #to keep only non-zero intervals\n",
    "filenames = WORDS['Filename'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6827df",
   "metadata": {},
   "source": [
    "# Separating AA and CA files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18bd23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_AA = []\n",
    "WORDS_CA = []\n",
    "for filename in filenames : \n",
    "    WORDS_file = WORDS[ WORDS['Filename']==filename][['Word','UtteranceName','Speaker', 'Length', 'Global_start', 'Filename', 'Global_end']]\n",
    "    WORDS_file.reset_index(drop=True, inplace=True)\n",
    "    WORDS_file=WORDS_file.sort_values('Speaker', ascending=True)\n",
    "    if filename[0:2]=='AA':\n",
    "        WORDS_AA.append(WORDS_file)\n",
    "    else:\n",
    "        WORDS_CA.append(WORDS_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b16e6",
   "metadata": {},
   "source": [
    "# Creating a csv file for each conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b7d5c",
   "metadata": {},
   "source": [
    "## Computing the \"syllables\" column, and grouping words by utterances (keeping the first start and last end, summing syllables and length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c88ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,WORDS in enumerate(WORDS_AA) :\n",
    "        \n",
    "    list_syll = []\n",
    "    for word in WORDS['Word']:\n",
    "        word=word.lower()\n",
    "        if word in dictionnary:\n",
    "            list_syll.append(dictionnary[word])\n",
    "        else : \n",
    "            list_syll.append(estimate_syll(word))\n",
    "            \n",
    "    WORDS[\"syllables\"] = list_syll\n",
    "    WORDS=WORDS.sort_values('Global_start', ascending=True)\n",
    "    WORDS_AA[i] = WORDS\n",
    "        \n",
    "    table = pd.pivot_table(WORDS, values = ['Length', 'Global_start', 'Global_end','syllables', 'Speaker', 'Filename', 'Word'], index = ['UtteranceName'], aggfunc = {'Length':np.sum, 'Global_start':min, 'syllables':np.sum, 'Speaker' : np.min, 'Filename':np.min, 'Word':'count', 'Global_end':max})\n",
    "    table = table.sort_values('Global_start', ascending = True)\n",
    "    if remove_silences:\n",
    "        table['Speech rate'] = table['syllables']/table['Length']\n",
    "    else:\n",
    "        table['Speech rate'] = table['syllables']/(table['Global_end']-table['Global_start'])\n",
    "    if remove_shorter:\n",
    "        table = table[ table['Word']>1]\n",
    "    WORDS_AA[i] = table\n",
    "    table.to_csv(join(path_AA, str(WORDS['Filename'][0])+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63215483",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,WORDS in enumerate(WORDS_CA) : \n",
    "        \n",
    "    list_syll = []\n",
    "    for word in WORDS['Word']:\n",
    "        word=str(word)\n",
    "        word = word.lower()\n",
    "        if word in dictionnary:\n",
    "            list_syll.append(dictionnary[word])\n",
    "        else : \n",
    "            list_syll.append(estimate_syll(word))\n",
    "    WORDS[\"syllables\"] = list_syll\n",
    "    WORDS_CA[i] = WORDS\n",
    "    \n",
    "    WORDS['Word'] = np.vectorize(str)(WORDS['Word'])\n",
    "    WORDS.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    table = pd.pivot_table(WORDS, values = ['Word', 'UtteranceName', 'Global_start', 'Global_end', 'Length', \"Filename\", \"syllables\"], index = ['UtteranceName', \"Speaker\"], aggfunc = {\"Global_end\":max,\"UtteranceName\":np.unique,'Word':'count', 'Global_start':min, \"Length\":sum, \"Speaker\":np.unique, \"Filename\":np.unique, \"syllables\":sum})    \n",
    "    table = table.sort_values('Global_start', ascending = True)\n",
    "    \n",
    "    if remove_silences :\n",
    "        table['Speech rate'] = table['syllables']/table['Length']\n",
    "    else : \n",
    "        table['Speech rate'] = table['syllables']/(table['Global_end']-table['Global_start'])\n",
    "    if remove_shorter:\n",
    "        table = table[ table['Word']>1]\n",
    "    WORDS_CA[i] = table\n",
    "    \n",
    "    table.to_csv(join(path_CA, str(WORDS['Filename'][0])+'.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3b1e1",
   "metadata": {},
   "source": [
    "# Importing the new csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af7b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-AN-DL.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-GD-DD.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-JL-AZ.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-LA-AN.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-LD-BF.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-MG-CH.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-MJ-CJ.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-ML-MP.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-XA-EH.csv\n"
     ]
    }
   ],
   "source": [
    "pathlist_AA = Path(path_AA).glob('**/*.csv')\n",
    "data_AA = []\n",
    "filenames_AA = []\n",
    "\n",
    "for path in pathlist_AA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        filenames_AA.append(str(path)[-12:-4])\n",
    "        data_AA.append(pd.read_csv(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dc9cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-AN-ZN.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-FB-MG.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-JL-JT.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-LD-GD.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-LJ-MJ.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-MB-LB.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-MD-GD.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-RL-ML.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-XA-LA.csv\n"
     ]
    }
   ],
   "source": [
    "pathlist_CA = Path(path_CA).glob('**/*.csv')\n",
    "data_CA = []\n",
    "filenames_CA = []\n",
    "\n",
    "for path in pathlist_CA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        filenames_CA.append(str(path)[-12:-4])\n",
    "        data_CA.append(pd.read_csv(path)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc093d",
   "metadata": {},
   "source": [
    "# Raw speech rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf4563a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs1 = plt.subplots(3, 3, figsize=(20,17))\n",
    "\n",
    "for i, data in enumerate(data_AA) :\n",
    "    \n",
    "    data_1 = data[ data['Speaker'] == \"Adult1\" ]\n",
    "    data_2 = data[ data['Speaker'] == \"Adult2\" ]\n",
    "    \n",
    "    axs1[i//3,i%3].set_title(data['Filename'][0])\n",
    "    axs1[i//3,i%3].set_xlabel('Time (sec)')\n",
    "    axs1[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs1[i//3,i%3].plot(data_1['Global_start'], data_1['Speech rate'], label=\"Adult1\", color=my_blue)\n",
    "    axs1[i//3,i%3].plot(data_2['Global_start'], data_2['Speech rate'], label=\"Adult2\", color=my_red)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig1)\n",
    "fig1.savefig(join(plots_path,\"Raw AA speech rate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad34c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots(3, 3, figsize=(20,17))\n",
    "\n",
    "for i, data in enumerate(data_CA) :\n",
    "    \n",
    "    data_1 = data[ data['Speaker'] == \"Parent\" ]\n",
    "    data_2 = data[ data['Speaker'] == \"Child\" ]\n",
    "    \n",
    "    axs2[i//3,i%3].set_title(data['Filename'][0])\n",
    "    axs2[i//3,i%3].set_xlabel('Time (sec)')\n",
    "    axs2[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs2[i//3,i%3].plot(data_1['Global_start'], data_1['Speech rate'], label=\"Parent\", color=my_blue)\n",
    "    axs2[i//3,i%3].plot(data_2['Global_start'], data_2['Speech rate'], label=\"Child\", color=my_red)\n",
    "    \n",
    "fig2.tight_layout()\n",
    "plt.close(fig2)\n",
    "fig2.savefig(join(plots_path,\"Raw CA speech rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6df9c",
   "metadata": {},
   "source": [
    "# Fourier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f89dca",
   "metadata": {},
   "source": [
    "# Smoothed speech rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84840082",
   "metadata": {},
   "source": [
    "## With a smoothing window of 10 utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b46870",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axs3 = plt.subplots(3, 3, figsize=(20,17))\n",
    "\n",
    "for i, table in enumerate(data_AA) :\n",
    "    \n",
    "    axs3[i//3,i%3].set_xlabel('time')\n",
    "    axs3[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs3[i//3,i%3].plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], 29, 3), label='Adult1',color = my_red)\n",
    "    axs3[i//3,i%3].plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], 29, 3), label='Adult2', color=my_blue)\n",
    "    axs3[i//3,i%3].set_title(str(table['Filename'][0]))\n",
    "    axs3[i//3,i%3].legend()\n",
    "    \n",
    "    if \"AA-LD-BF\" in filenames_AA[i]:\n",
    "        fig35 = plt.figure()\n",
    "        plt.plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], 29, 3), label='Adult1',color = my_red)\n",
    "        plt.plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], 29, 3), label='Adult2', color=my_blue)\n",
    "        plt.xlabel(\"time (s)\")\n",
    "        plt.ylabel(\"Speech rate (syllables/s)\")\n",
    "        plt.close(fig35)\n",
    "        fig35.savefig(join(plots_path,\"Smoothed AA\"))\n",
    "    \n",
    "fig3.tight_layout()\n",
    "plt.close(fig3)\n",
    "fig3.savefig(join(plots_path,\"Smoothed AA speech rate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39380d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, axs4 = plt.subplots(3, 3, figsize=(20,15))\n",
    "\n",
    "for i, table in enumerate(data_CA) :\n",
    "    \n",
    "    axs4[i//3,i%3].set_xlabel('time')\n",
    "    axs4[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs4[i//3,i%3].plot(table[table['Speaker']=='Parent']['Global_start'], smooth_2(table[table['Speaker']=='Parent']['Speech rate'],29, 3), label='Parent',color = my_red)\n",
    "    axs4[i//3,i%3].plot( table[table['Speaker']=='Child']['Global_start'], smooth_2(table[table['Speaker']=='Child']['Speech rate'], 29, 3), label='Child', color=my_blue)\n",
    "    axs4[i//3,i%3].set_title(str(table['Filename'][0]))\n",
    "    axs4[i//3,i%3].legend()\n",
    "    \n",
    "    if \"CA-LD-GD\" in filenames_CA[i]:\n",
    "        fig45 = plt.figure()\n",
    "        plt.plot(table[table['Speaker']=='Parent']['Global_start'], smooth_2(table[table['Speaker']=='Parent']['Speech rate'], 29, 3), label='Parent',color = my_red)\n",
    "        plt.plot( table[table['Speaker']=='Child']['Global_start'], smooth_2(table[table['Speaker']=='Child']['Speech rate'], 29, 3), label='Child', color=my_blue)\n",
    "        plt.xlabel(\"time (s)\")\n",
    "        plt.ylabel(\"Speech rate (syllables/s)\")\n",
    "        plt.close(fig45)\n",
    "        fig45.savefig(join(plots_path,\"Smoothed CA\"))\n",
    "    \n",
    "fig4.tight_layout()\n",
    "plt.close(fig4)\n",
    "fig4.savefig(join(plots_path,\"Smoothed CA speech rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5286b5",
   "metadata": {},
   "source": [
    "# \"Adapted\" speech rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be785b00",
   "metadata": {},
   "source": [
    "## To compute the correlation, the time series must have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "807caaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "listes_roles = []\n",
    "for i, data in enumerate(data_CA):\n",
    "    liste_role = []\n",
    "    for i in range(len(data)):\n",
    "        liste_role.append((data['Speaker'][i], data['Global_start'][i]))\n",
    "    listes_roles.append(liste_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e61d08",
   "metadata": {},
   "source": [
    "### Loading manual phases notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bd8cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_AA = open(join(input_path, \"Corrected transcriptions\\\\MY_ANNOTATIONS_AA.txt\"), 'r').read()\n",
    "\n",
    "lines_AA = text_AA.split('\\n')\n",
    "Filenames_AA = []\n",
    "Phases_AA = []\n",
    "line = lines_AA[0]\n",
    "file = []\n",
    "for line in lines_AA:\n",
    "    if line!='' and line!=' ':\n",
    "        if \"END\" not in line :\n",
    "            if \"FILENAME\" not in line:\n",
    "                label, time_string = line.split('[')\n",
    "                time_string = time_string.replace('[', '')\n",
    "                time_string = time_string.replace(']', '')\n",
    "                time_string = time_string.replace(' ', '')\n",
    "                time = conversion_time(time_string)\n",
    "                file.append([label, time])\n",
    "            else:\n",
    "                NOM, filename = line.split(' ')\n",
    "        else:\n",
    "            Filenames_AA.append(filename)\n",
    "            Phases_AA.append(file)\n",
    "            file=[]\n",
    "phases_AA = pd.DataFrame(list(zip(Filenames_AA, Phases_AA)), columns = ['Filename', 'Phases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c34f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_CA = open(join(input_path, \"Corrected transcriptions\\\\MY_ANNOTATIONS_CA.txt\"), 'r').read()\n",
    "\n",
    "lines_CA = text_CA.split('\\n')\n",
    "Filenames_CA = []\n",
    "Phases_CA = []\n",
    "line = lines_CA[0]\n",
    "file = []\n",
    "for line in lines_CA:\n",
    "    if line!='' and line!=' ':\n",
    "        if \"END\" not in line :\n",
    "            if \"FILENAME\" not in line:\n",
    "                label, time_string = line.split('[')\n",
    "                time_string = time_string.replace('[', '')\n",
    "                time_string = time_string.replace(']', '')\n",
    "                time_string = time_string.replace(' ', '')\n",
    "                time = conversion_time(time_string)\n",
    "                file.append([label, time])\n",
    "            else:\n",
    "                NOM, filename = line.split(' ')\n",
    "        else:\n",
    "            Filenames_CA.append(filename)\n",
    "            Phases_CA.append(file)\n",
    "            file=[]\n",
    "phases_CA = pd.DataFrame(list(zip(Filenames_CA, Phases_CA)), columns = ['Filename', 'Phases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25465f43",
   "metadata": {},
   "source": [
    "### Adding phases, roles and smoothed columns, save as csv, and adapt the number of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5fd04cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA-AN-DL\n",
      "AA-GD-DD\n",
      "AA-JL-AZ\n",
      "AA-LA-AN\n",
      "AA-LD-BF\n",
      "AA-MG-CH\n",
      "AA-MJ-CJ\n",
      "AA-ML-MP\n",
      "AA-XA-EH\n"
     ]
    }
   ],
   "source": [
    "for i,filename in enumerate(filenames_AA):\n",
    "    \n",
    "    print(filename)\n",
    "    \n",
    "    data = data_AA[i]\n",
    "    \n",
    "    adapted_data = adapt_points_3(data,\"Speaker\", \"Global_start\", \"Speech rate\")\n",
    "    \n",
    "    liste_phases = list(phases_AA[phases_AA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    adapted_data['Phases'] = find_phases(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data['Roles'] = find_roles(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data[\"Word\"] = data[\"Word\"]\n",
    "    \n",
    "    adapted_data[\"smoothed\"] = smoothed_column(adapted_data, default_window, 'AA')\n",
    "    \n",
    "    adapted_data.to_csv(join(path_AA,str(filename)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43c8b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA-AN-ZN\n",
      "CA-FB-MG\n",
      "CA-JL-JT\n",
      "CA-LD-GD\n",
      "CA-LJ-MJ\n",
      "CA-MB-LB\n",
      "CA-MD-GD\n",
      "CA-RL-ML\n",
      "CA-XA-LA\n"
     ]
    }
   ],
   "source": [
    "for i,filename in enumerate(filenames_CA):\n",
    "    \n",
    "    print(filename)\n",
    "    \n",
    "    data = data_CA[i]\n",
    "    \n",
    "    adapted_data = adapt_points_3(data,\"Speaker\", \"Global_start\", \"Speech rate\")\n",
    "    \n",
    "    liste_phases = list(phases_CA[phases_CA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    adapted_data['Phases'] = find_phases(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data['Roles'] = find_roles(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data[\"Word\"] = data[\"Word\"]\n",
    "    \n",
    "    adapted_data[\"smoothed\"] = smoothed_column(adapted_data, default_window, 'CA')\n",
    "        \n",
    "    adapted_data.to_csv(join(path_CA,str(filename)+'.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c59e65",
   "metadata": {},
   "source": [
    "### Importing the new csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37ae507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-AN-DL.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-GD-DD.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-JL-AZ.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-LA-AN.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-LD-BF.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-MG-CH.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-MJ-CJ.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-ML-MP.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\AA\\AA-XA-EH.csv\n",
      "[     Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0      9.160000  Adult1     5.691489   PREGAME     0    83   \n",
      "1             1     29.590000  Adult2     7.500000   PREGAME     0     2   \n",
      "2             2     30.060000  Adult1     6.611570   PREGAME     0     7   \n",
      "3             3     32.160000  Adult2     1.483051   PREGAME     0     6   \n",
      "4             4     34.110000  Adult1     1.311475   PREGAME     0     4   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "141         141    931.080000  Adult2     4.020101  POSTGAME     0     6   \n",
      "142         142    951.200000  Adult1     3.065134  POSTGAME     0    12   \n",
      "143         143    947.880000  Adult2     0.390320  POSTGAME     0     4   \n",
      "144         144    966.290000  Adult1    10.294118  POSTGAME     0     5   \n",
      "145         145    964.506667  Adult2     7.641775  POSTGAME     0    47   \n",
      "\n",
      "     smoothed  \n",
      "0    1.998147  \n",
      "1    3.032931  \n",
      "2    2.474338  \n",
      "3    4.108200  \n",
      "4    3.114748  \n",
      "..        ...  \n",
      "141  2.297373  \n",
      "142  3.350379  \n",
      "143  1.931073  \n",
      "144  2.898068  \n",
      "145  1.803930  \n",
      "\n",
      "[146 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0          4.52  Adult1     1.445783   PREGAME     0     7   \n",
      "1             1          2.46  Adult2     5.696203   PREGAME     0    15   \n",
      "2             2         18.21  Adult1     3.041397   PREGAME     0    30   \n",
      "3             3          8.56  Adult2     0.775923   PREGAME     0    86   \n",
      "4             4         54.68  Adult1     0.336700   PREGAME     0     2   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "115         115        896.05  Adult2     2.197802  POSTGAME     0    29   \n",
      "116         116        924.32  Adult1     1.748252  POSTGAME     0    11   \n",
      "117         117        902.91  Adult2     4.138988  POSTGAME     0    21   \n",
      "118         118        936.74  Adult1     3.030303  POSTGAME     0     3   \n",
      "119         119        927.10  Adult2     1.038062  POSTGAME     0    14   \n",
      "\n",
      "     smoothed  \n",
      "0    1.031268  \n",
      "1    2.088039  \n",
      "2    1.259304  \n",
      "3    2.110792  \n",
      "4    1.840277  \n",
      "..        ...  \n",
      "115  2.075232  \n",
      "116  2.392540  \n",
      "117  1.676031  \n",
      "118  2.093272  \n",
      "119  1.511178  \n",
      "\n",
      "[120 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0         1.960  Adult1     5.633803   PREGAME     0    10   \n",
      "1             1         4.610  Adult2     6.451613   PREGAME     0     8   \n",
      "2             2         7.525  Adult1     8.840996   PREGAME     0     2   \n",
      "3             3        16.490  Adult2     6.467662   PREGAME     0    15   \n",
      "4             4        19.010  Adult1     5.555556   PREGAME     0    10   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "159         159       931.100  Adult2     3.125000  POSTGAME     0     5   \n",
      "160         160       940.660  Adult1     0.498753  POSTGAME     0     2   \n",
      "161         161       942.680  Adult2     6.250000  POSTGAME     0     4   \n",
      "162         162       952.060  Adult1     6.077348  POSTGAME     0    32   \n",
      "163         163       946.000  Adult2     2.754237  POSTGAME     0    11   \n",
      "\n",
      "     smoothed  \n",
      "0    2.964569  \n",
      "1    2.895071  \n",
      "2    3.599490  \n",
      "3    3.388898  \n",
      "4    3.946712  \n",
      "..        ...  \n",
      "159  3.110620  \n",
      "160  1.830192  \n",
      "161  2.742296  \n",
      "162  1.342310  \n",
      "163  2.204790  \n",
      "\n",
      "[164 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0         11.57  Adult1     1.457726   PREGAME     0     3   \n",
      "1             1          1.90  Adult2     0.817996   PREGAME     0     4   \n",
      "2             2         15.24  Adult1     3.519062   PREGAME     0     2   \n",
      "3             3         14.00  Adult2     0.796813   PREGAME     0    11   \n",
      "4             4         20.53  Adult1     3.794370   PREGAME     0    25   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "101         101        899.61  Adult2     3.271205  POSTGAME     0    45   \n",
      "102         102        955.08  Adult1     2.416212  POSTGAME     0    21   \n",
      "103         103        960.00  Adult2     3.692308  POSTGAME     0    28   \n",
      "104         104        982.35  Adult1     0.627003  POSTGAME     0     3   \n",
      "105         105        986.12  Adult2     2.772277  POSTGAME     0    21   \n",
      "\n",
      "     smoothed  \n",
      "0    1.672336  \n",
      "1    1.635488  \n",
      "2    2.331470  \n",
      "3    2.118580  \n",
      "4    2.449360  \n",
      "..        ...  \n",
      "101  2.291652  \n",
      "102  0.790011  \n",
      "103  1.999368  \n",
      "104  0.750187  \n",
      "105  1.475807  \n",
      "\n",
      "[106 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases      Roles  Word  \\\n",
      "0             0      23.89625  Adult1     5.267050   PREGAME          0     2   \n",
      "1             1      47.38000  Adult2     6.451613      GAME  SPEAKER1     12   \n",
      "2             2      53.01000  Adult1     0.683995      GAME  SPEAKER1     11   \n",
      "3             3      48.07000  Adult2     2.588235      GAME  SPEAKER1     40   \n",
      "4             4      65.90000  Adult1     0.591716      GAME  SPEAKER1     15   \n",
      "..          ...           ...     ...          ...       ...        ...   ...   \n",
      "181         181     979.78000  Adult2     7.884284  POSTGAME          0     5   \n",
      "182         182    1020.26000  Adult1     3.703704  POSTGAME          0    11   \n",
      "183         183    1012.27000  Adult2     2.958580  POSTGAME          0     2   \n",
      "184         184    1025.25500  Adult1     5.456278  POSTGAME          0    24   \n",
      "185         185    1032.00000  Adult2    10.256410  POSTGAME          0    13   \n",
      "\n",
      "     smoothed  \n",
      "0    2.917203  \n",
      "1    2.431774  \n",
      "2    3.265879  \n",
      "3    3.212747  \n",
      "4    3.743309  \n",
      "..        ...  \n",
      "181  5.298231  \n",
      "182  2.429359  \n",
      "183  5.264017  \n",
      "184  2.066758  \n",
      "185  4.698035  \n",
      "\n",
      "[186 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0          2.35  Adult1     1.658537   PREGAME     0    15   \n",
      "1             1         13.40  Adult2     4.761905   PREGAME     0    12   \n",
      "2             2         15.87  Adult1     1.398601   PREGAME     0     8   \n",
      "3             3         23.64  Adult2     0.281373   PREGAME     0     5   \n",
      "4             4         26.00  Adult1     3.586863   PREGAME     0    61   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "101         101        902.03  Adult2     6.000000  POSTGAME     0     7   \n",
      "102         102        914.41  Adult1     4.605937  POSTGAME     0    11   \n",
      "103         103        904.82  Adult2     6.315789  POSTGAME     0     2   \n",
      "104         104        928.01  Adult1     8.292683  POSTGAME     0    17   \n",
      "105         105        925.38  Adult2     4.848485  POSTGAME     0    79   \n",
      "\n",
      "     smoothed  \n",
      "0    1.359866  \n",
      "1    1.758857  \n",
      "2    1.564679  \n",
      "3    2.609468  \n",
      "4    2.339873  \n",
      "..        ...  \n",
      "101  3.015035  \n",
      "102  2.435627  \n",
      "103  2.984923  \n",
      "104  2.349016  \n",
      "105  2.539063  \n",
      "\n",
      "[106 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0      3.323333  Adult1     4.679158   PREGAME     0     2   \n",
      "1             1     11.490000  Adult2     4.245283   PREGAME     0     4   \n",
      "2             2     13.340000  Adult1     3.365385   PREGAME     0    22   \n",
      "3             3     22.740000  Adult2     4.615385   PREGAME     0     7   \n",
      "4             4     26.930000  Adult1     3.675345   PREGAME     0    22   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "145         145   1047.080000  Adult2     3.488372  POSTGAME     0     3   \n",
      "146         146   1051.600000  Adult1     5.704100  POSTGAME     0     3   \n",
      "147         147   1051.310000  Adult2     6.666667  POSTGAME     0     3   \n",
      "148         148   1070.995000  Adult1     4.345889  POSTGAME     0     9   \n",
      "149         149   1058.570000  Adult2     4.917461  POSTGAME     0    58   \n",
      "\n",
      "     smoothed  \n",
      "0    2.257306  \n",
      "1    2.714693  \n",
      "2    2.699722  \n",
      "3    3.111518  \n",
      "4    3.287700  \n",
      "..        ...  \n",
      "145  3.290694  \n",
      "146  3.169434  \n",
      "147  2.859855  \n",
      "148  2.623232  \n",
      "149  2.668559  \n",
      "\n",
      "[150 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0          0.00  Adult1     4.867257   PREGAME     0     8   \n",
      "1             1          2.35  Adult2     3.508772   PREGAME     0     2   \n",
      "2             2          3.23  Adult1     4.504505   PREGAME     0     4   \n",
      "3             3          4.84  Adult2     4.558824   PREGAME     0    25   \n",
      "4             4         12.39  Adult1     3.590426   PREGAME     0    19   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "115         115        980.77  Adult2     4.285714  POSTGAME     0     3   \n",
      "116         116        991.61  Adult1     1.898102  POSTGAME     0    25   \n",
      "117         117        988.03  Adult2     4.583333  POSTGAME     0    49   \n",
      "118         118       1026.35  Adult1     3.577982  POSTGAME     0    21   \n",
      "119         119        993.85  Adult2     1.733159  POSTGAME     0    61   \n",
      "\n",
      "     smoothed  \n",
      "0    2.555816  \n",
      "1    2.633027  \n",
      "2    3.250260  \n",
      "3    2.911676  \n",
      "4    3.912512  \n",
      "..        ...  \n",
      "115  2.844978  \n",
      "116  2.453658  \n",
      "117  2.743661  \n",
      "118  1.966405  \n",
      "119  2.513862  \n",
      "\n",
      "[120 rows x 8 columns],      Unnamed: 0  Global_start Speaker  Speech rate    Phases Roles  Word  \\\n",
      "0             0        0.0000  Adult1     4.605263   PREGAME     0     6   \n",
      "1             1        2.3900  Adult2     4.838710   PREGAME     0     2   \n",
      "2             2        5.0700  Adult1     5.000000   PREGAME     0    13   \n",
      "3             3        8.3800  Adult2     0.688073   PREGAME     0     4   \n",
      "4             4        9.5700  Adult1     4.159664   PREGAME     0    77   \n",
      "..          ...           ...     ...          ...       ...   ...   ...   \n",
      "219         219     1195.5800  Adult2     5.369128  POSTGAME     0     2   \n",
      "220         220     1220.7700  Adult1     0.000000  POSTGAME     0     3   \n",
      "221         221     1212.5225  Adult2     6.020410  POSTGAME     0     4   \n",
      "222         222     1224.4600  Adult1     3.553166  POSTGAME     0    21   \n",
      "223         223     1226.1700  Adult2     0.186916  POSTGAME     0    72   \n",
      "\n",
      "     smoothed  \n",
      "0    2.825361  \n",
      "1    2.485124  \n",
      "2    3.451339  \n",
      "3    2.893621  \n",
      "4    4.362085  \n",
      "..        ...  \n",
      "219  2.565951  \n",
      "220  2.082124  \n",
      "221  2.543806  \n",
      "222  1.567634  \n",
      "223  2.313694  \n",
      "\n",
      "[224 rows x 8 columns]]\n"
     ]
    }
   ],
   "source": [
    "pathlist_AA = Path(path_AA).glob('**/*.csv')\n",
    "adapted_data_AA = []\n",
    "adapted_filenames_AA = []\n",
    "\n",
    "for path in pathlist_AA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        adapted_filenames_AA.append(str(path)[-12:-4])\n",
    "        adapted_data_AA.append(pd.read_csv(path))\n",
    "print(adapted_data_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1f992e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-AN-ZN.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-FB-MG.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-JL-JT.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-LD-GD.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-LJ-MJ.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-MB-LB.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-MD-GD.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-RL-ML.csv\n",
      "input\\speech_rate\\Corrected transcriptions\\CA\\CA-XA-LA.csv\n"
     ]
    }
   ],
   "source": [
    "pathlist_CA = Path(path_CA).glob('**/*.csv')\n",
    "adapted_data_CA = []\n",
    "adapted_filenames_CA = []\n",
    "\n",
    "for path in pathlist_CA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        adapted_filenames_CA.append(str(path)[-12:-4])\n",
    "        adapted_data_CA.append(pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261c00b",
   "metadata": {},
   "source": [
    "### Plot of the adapted speech rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fdec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5, axs5 = plt.subplots(9, 2, figsize=(15,20))\n",
    "\n",
    "for i, adapted_data in enumerate(adapted_data_AA) :\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Adult1\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Adult2\" ]  \n",
    "\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Adult1\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Adult2\" ]\n",
    "    \n",
    "    axs5[i,0].set_title(filename+\" speaker1\")\n",
    "    axs5[i,0].set_xlabel('Time (sec)')\n",
    "    axs5[i,0].set_ylabel('Speech rate')\n",
    "    axs5[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'], label=\"Raw\", color=my_blue)\n",
    "    axs5[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'],'o', label=\"Adapted\", color=my_red)\n",
    "    \n",
    "    axs5[i,1].set_title(filename+\" speaker2\")\n",
    "    axs5[i,1].set_xlabel('Time (sec)')\n",
    "    axs5[i,1].set_ylabel('Adapted speech rate')\n",
    "    axs5[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'],label=\"Raw\", color=my_blue)\n",
    "    axs5[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'], 'o',label=\"Adapted\", color=my_red)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig5)\n",
    "fig5.savefig(join(plots_path,\"Adapted AA speech rate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e512bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6, axs6 = plt.subplots(9, 2, figsize=(15,20))\n",
    "\n",
    "for i, adapted_data in enumerate(adapted_data_CA) :\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Parent\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Child\" ]  \n",
    "\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Parent\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Child\" ]\n",
    "    \n",
    "    axs6[i,0].set_title(filename+\" speaker1\")\n",
    "    axs6[i,0].set_xlabel('Time (sec)')\n",
    "    axs6[i,0].set_ylabel('Speech rate')\n",
    "    axs6[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'], label=\"Raw\", color=my_blue)\n",
    "    axs6[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'],'o', label=\"Adapted\", color=my_red)\n",
    "    \n",
    "    axs6[i,1].set_title(filename+\" speaker2\")\n",
    "    axs6[i,1].set_xlabel('Time (sec)')\n",
    "    axs6[i,1].set_ylabel('Adapted speech rate')\n",
    "    axs6[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'],label=\"Raw\", color=my_blue)\n",
    "    axs6[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'], 'o',label=\"Adapted\", color=my_red)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig6)\n",
    "fig6.savefig(join(plots_path,\"Adapted CA speech rate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145896df",
   "metadata": {},
   "source": [
    "# Global correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "263adb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_AA = []\n",
    "p_values_AA = []\n",
    "\n",
    "for data in adapted_data_AA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "    \n",
    "    data_1 = data[data['Speaker']==speaker1]['Speech rate']\n",
    "    data_2 = data[data['Speaker']==speaker2]['Speech rate']\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(np.array(data_1), np.array(data_2))\n",
    "    correlation_list_AA.append(correlation)\n",
    "    p_values_AA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a93155f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "    \n",
    "    corr = correlation_list_AA[i]\n",
    "    p_value = p_values_AA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o', label=filename, color=my_colors[i])\n",
    "    plt.plot([\"p_value\"], [p_value], 'o',color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig7)\n",
    "fig7.savefig(join(plots_path,\"Global AA correlation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "317dbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_CA = []\n",
    "p_values_CA = []\n",
    "\n",
    "for data in adapted_data_CA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "    \n",
    "    data_1 = data[data['Speaker']==speaker1]['Speech rate']\n",
    "    data_2 = data[data['Speaker']==speaker2]['Speech rate']\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(np.array(data_1), np.array(data_2))\n",
    "    correlation_list_CA.append(correlation)\n",
    "    p_values_CA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e02191da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "    \n",
    "    corr = correlation_list_CA[i]\n",
    "    p_value = p_values_CA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o',color=my_colors[i], label=filename)\n",
    "    plt.plot([\"p_value\"], [p_value], 'o', color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig8)\n",
    "fig8.savefig(join(plots_path,\"Global CA correlation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84780e4",
   "metadata": {},
   "source": [
    "## Global correlation with default window smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7ff37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_AA = []\n",
    "p_values_AA = []\n",
    "\n",
    "for data in adapted_data_AA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "\n",
    "    data_1 = data[data['Speaker']==speaker1]\n",
    "    data_2 = data[data['Speaker']==speaker2]\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(smooth_2(data_1['Speech rate'], default_window, default_poly), smooth_2(data_2['Speech rate'], default_window, default_poly))\n",
    "\n",
    "    correlation_list_AA.append(correlation)\n",
    "    p_values_AA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e2b6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig9 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "    \n",
    "    corr = correlation_list_AA[i]\n",
    "    p_value = p_values_AA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o', label=filename, color=my_colors[i])\n",
    "    plt.plot([\"p_value\"], [p_value], 'o',color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig9)\n",
    "fig9.savefig(join(plots_path,\"Global AA smoothed correlation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97621c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_CA = []\n",
    "p_values_CA = []\n",
    "\n",
    "for data in adapted_data_CA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "\n",
    "    data_1 = data[data['Speaker']==speaker1]\n",
    "    data_2 = data[data['Speaker']==speaker2]\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(smooth_2(data_1['Speech rate'], default_window, default_poly), smooth_2(data_2['Speech rate'], default_window, default_poly))\n",
    "\n",
    "    correlation_list_CA.append(correlation)\n",
    "    p_values_CA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3973bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig10 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "    \n",
    "    corr = correlation_list_CA[i]\n",
    "    p_value = p_values_CA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o', label=filename, color=my_colors[i])\n",
    "    plt.plot([\"p_value\"], [p_value], 'o',color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig10)\n",
    "fig10.savefig(join(plots_path,\"Global CA smoothed correlation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaa7b4",
   "metadata": {},
   "source": [
    "# Influence of the smoothing window on global correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70597100",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = np.arange(5, 25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1bf7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_AA = []\n",
    "p_values_AA = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    correlation_file = []\n",
    "    p_value_file = []\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    data1  = data[data['Speaker']=='Adult1']\n",
    "    data2  = data[data['Speaker']=='Adult2']\n",
    "    \n",
    "    for window in windows :\n",
    "        \n",
    "        corr, p_value = stats.pearsonr(smooth_2(data1['Speech rate'], window, default_poly), smooth_2(data2['Speech rate'], window, default_poly))\n",
    "        correlation_file.append(corr)\n",
    "        p_value_file.append(p_value)\n",
    "        \n",
    "    correlations_AA.append(correlation_file)\n",
    "    p_values_AA.append(p_value_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5e058cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig11 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "\n",
    "    plt.plot(windows, correlations_AA[i], label=filename, color=my_colors[i])\n",
    "    #plt.plot(windows, p_values_AA[i], '--',color=my_colors[i])\n",
    "\n",
    "plt.xlabel(\"smoothing window\")\n",
    "plt.ylabel(\"correlation\")\n",
    "plt.legend()\n",
    "plt.close(fig11)\n",
    "fig11.savefig(join(plots_path,\"Window influence on correlation AA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52026186",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_CA = []\n",
    "p_values_CA = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    correlation_file = []\n",
    "    p_values_file = []\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    data1  = data[data['Speaker']=='Parent']\n",
    "    data2  = data[data['Speaker']=='Child']\n",
    "    \n",
    "    for window in windows :\n",
    "        \n",
    "        corr, p_value = stats.pearsonr(smooth_2(data1['Speech rate'], window, default_poly), smooth_2(data2['Speech rate'], window, default_poly))\n",
    "        correlation_file.append(corr)\n",
    "        p_values_file.append(p_value)\n",
    "        \n",
    "    correlations_CA.append(correlation_file)\n",
    "    p_values_CA.append(p_values_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6cc925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig12 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "\n",
    "    plt.plot(windows, correlations_CA[i], label=filename, color=my_colors[i])\n",
    "    #plt.plot(windows, p_values_CA[i], \"--\", color=my_colors[i] )\n",
    "    \n",
    "plt.xlabel(\"smoothing window\")\n",
    "plt.ylabel(\"correlation\")\n",
    "plt.legend(loc=4)\n",
    "plt.close(fig12)\n",
    "fig12.savefig(join(plots_path,\"Window influence on correlation CA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019245ac",
   "metadata": {},
   "source": [
    "# Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bd9d1",
   "metadata": {},
   "source": [
    "## The conversation contains three phases : pregame conversation (explanation of the rules, often a monologue), the guessing game, and postgame conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e5373",
   "metadata": {},
   "source": [
    "### Computing the correlation of each phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b69ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phases = ['PREGAME', 'GAME', 'POSTGAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e18b6c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA-AN-DL\n",
      "AA-GD-DD\n",
      "AA-JL-AZ\n",
      "AA-LA-AN\n",
      "AA-LD-BF\n",
      "AA-MG-CH\n",
      "AA-MJ-CJ\n",
      "AA-ML-MP\n",
      "AA-XA-EH\n"
     ]
    }
   ],
   "source": [
    "list_corr_files_AA = []\n",
    "nb_points_files_AA = []\n",
    "p_values_files_AA = []\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "\n",
    "    print(filename)\n",
    "    \n",
    "    list_corr = []\n",
    "        \n",
    "    df = pd.read_csv(path_AA+str(filename)+'.csv')\n",
    "    \n",
    "    corr_phases, nb_points, p_values = compute_corr_phases(df, Phases, \"Phases\", \"smoothed\", \"Speaker\", \"Global_start\")\n",
    "\n",
    "    p_values_files_AA.append(p_values)        \n",
    "    list_corr_files_AA.append(corr_phases)\n",
    "    nb_points_files_AA.append(nb_points)\n",
    "\n",
    "\n",
    "mean_corr = [my_mean([list_corr_files_AA[j][i] for i in range(len(list_corr_files_AA[j]))]) for j in range(len(list_corr_files_AA))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6662fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corr_files_CA = []\n",
    "nb_points_files_CA = []\n",
    "p_values_files_CA = []\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "    list_corr = []\n",
    "        \n",
    "    file = data_CA[i]\n",
    "\n",
    "    df = pd.read_csv(path_CA+str(filename)+'.csv')\n",
    "    \n",
    "    corr_phases, nb_points, p_values = compute_corr_phases(df, Phases, \"Phases\", \"smoothed\", \"Speaker\", \"Global_start\")\n",
    "\n",
    "    p_values_files_CA.append(p_values)\n",
    "    list_corr_files_CA.append(corr_phases)\n",
    "    nb_points_files_CA.append(nb_points)\n",
    "\n",
    "mean_corr = [my_mean([list_corr_files_CA[j][i] for i in range(len(list_corr_files_CA[j]))]) for j in range(len(list_corr_files_CA))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eaea13",
   "metadata": {},
   "source": [
    "### Plotting phases on the speech rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4acde0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig13, axs13 = plt.subplots(9, 1, figsize=(15,20))\n",
    "\n",
    "for i, table in enumerate(data_AA) :\n",
    "    \n",
    "    filename = filenames_AA[i]\n",
    "    \n",
    "    axs13[i].set_xlabel('time')\n",
    "    axs13[i].set_ylabel('Speech rate')\n",
    "    axs13[i].plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], default_window, default_poly), label='Adult1',color = my_red)\n",
    "    axs13[i].plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], default_window, default_poly), label='Adult2', color=my_blue)\n",
    "    axs13[i].set_title(str(table['Filename'][0]))\n",
    "    axs13[i].legend()\n",
    "    \n",
    "    liste_phases = list(phases_AA[phases_AA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    for couple in liste_phases:\n",
    "        \n",
    "        label, time = couple\n",
    "\n",
    "        if \"1\" in label:\n",
    "            axs13[i].axvline(x=time, color='r')\n",
    "        elif \"2\" in label:\n",
    "            axs13[i].axvline(x=time, color='b')\n",
    "        else :\n",
    "            axs13[i].axvline(x=time, color='orange')\n",
    "            \n",
    "    if \"AA-MG-CH\" in filename:\n",
    "        fig135 = plt.figure(figsize=(20,5))\n",
    "        plt.plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], default_window, default_poly), label='Adult1',color = my_red)\n",
    "        plt.plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], default_window, default_poly), label='Adult2', color=my_blue)\n",
    "        \n",
    "        for couple in liste_phases:\n",
    "        \n",
    "            label, time = couple\n",
    "\n",
    "            if \"1\" in label:\n",
    "                plt.axvline(x=time, color='r')\n",
    "            elif \"2\" in label:\n",
    "                plt.axvline(x=time, color='b')\n",
    "            else :\n",
    "                plt.axvline(x=time, color='orange')\n",
    "        \n",
    "        plt.close(fig135)\n",
    "        fig135.savefig(examples_path + \"Phases_SR_AA\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close(fig13)\n",
    "fig13.savefig(join(plots_path,\"Phases and smoothed speech rate AA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ec930cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig14, axs14 = plt.subplots(9, 1, figsize=(15,20))\n",
    "\n",
    "for i, table in enumerate(data_CA) :\n",
    "    \n",
    "    filename = filenames_CA[i]\n",
    "    \n",
    "    axs14[i].set_xlabel('time')\n",
    "    axs14[i].set_ylabel('Speech rate')\n",
    "    axs14[i].plot(table[table['Speaker']=='Parent']['Global_start'], smooth_2(table[table['Speaker']=='Parent']['Speech rate'], default_window, default_poly), label='Parent',color = my_red)\n",
    "    axs14[i].plot( table[table['Speaker']=='Child']['Global_start'], smooth_2(table[table['Speaker']=='Child']['Speech rate'], default_window, default_poly), label='Child', color=my_blue)\n",
    "    axs14[i].set_title(str(table['Filename'][0]))\n",
    "    axs14[i].legend()\n",
    "    \n",
    "    liste_phases = list(phases_CA[phases_CA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    for couple in liste_phases:\n",
    "        \n",
    "        label, time = couple\n",
    "\n",
    "        if \"1\" in label:\n",
    "            axs14[i].axvline(x=time, color='r')\n",
    "        elif \"2\" in label:\n",
    "            axs14[i].axvline(x=time, color='b')\n",
    "        else :\n",
    "            axs14[i].axvline(x=time, color='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close(fig14)\n",
    "fig14.savefig(join(plots_path,\"Phases and smoothed speech rate CA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252173c",
   "metadata": {},
   "source": [
    "### Plotting correlation of each phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7aba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig15, axs15 = plt.subplots(3,1, figsize=(10,10))\n",
    "ax1, ax2, ax3 = axs15\n",
    "\n",
    "for i, list_corr in enumerate(list_corr_files_AA):\n",
    "    \n",
    "    ax1.plot(Phases, list_corr, 'o',label=str(filenames_AA[i]))\n",
    "    \n",
    "    ax2.plot(Phases, nb_points_files_AA[i],'o', label=str(filenames_AA[i]))\n",
    "    \n",
    "    ax3.plot(Phases, p_values_files_AA[i],'o', label=str(filenames_AA[i]))\n",
    "    \n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "\n",
    "ax1.set_ylabel(\"correlation\")\n",
    "ax2.set_ylabel(\"number of utterances points\")\n",
    "ax3.set_ylabel(\"p_value\")\n",
    "\n",
    "ax1.legend(loc=(0.2, 0.05))\n",
    "ax2.legend(loc=(0.2, 0.05))\n",
    "ax3.legend(loc=(0.2, 0.05))\n",
    "\n",
    "plt.close(fig15)\n",
    "fig15.savefig(join(plots_path,\"Correlation by phase AA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86246934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig16, axs16 = plt.subplots(3,1, figsize=(10,10))\n",
    "ax1, ax2, ax3 = axs16\n",
    "\n",
    "for i, list_corr in enumerate(list_corr_files_CA):\n",
    "    \n",
    "    ax1.plot(Phases, list_corr, 'o',label=str(filenames_CA[i]))\n",
    "    \n",
    "    ax2.plot(Phases, nb_points_files_CA[i], 'o',label=str(filenames_CA[i]))\n",
    "    \n",
    "    ax3.plot(Phases, p_values_files_CA[i],'o', label=str(filenames_CA[i]))\n",
    "    \n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "\n",
    "ax1.set_ylabel(\"correlation\")\n",
    "ax2.set_ylabel(\"number of utterances points\")\n",
    "ax3.set_ylabel(\"p_value\")\n",
    "\n",
    "ax1.legend(loc=(0.2, 0.05))\n",
    "ax2.legend(loc=(0.2, 0.05))\n",
    "ax3.legend(loc=(0.2, 0.05))\n",
    "\n",
    "plt.close(fig16)\n",
    "fig16.savefig(join(plots_path,\"Correlation by phase CA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c69624",
   "metadata": {},
   "source": [
    "# Correlation by game role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e6358",
   "metadata": {},
   "source": [
    "## Each speaker is alternatively asking or answering the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "734f0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roles_AA, Correlations_AA, p_values_AA = [],[], []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "\n",
    "    filename = filenames_AA[i]\n",
    "    \n",
    "    roles, correlations, p_values = compute_corr_roles(data, \"Roles\", \"Speech rate\", \"Speaker\", \"Global_start\")\n",
    "    \n",
    "    Roles_AA.append(roles)\n",
    "    Correlations_AA.append(correlations)\n",
    "    p_values_AA.append(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7852249",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig17, axs17 = plt.subplots(3,3, figsize=(10,10))\n",
    "\n",
    "\n",
    "for i in range(len(Roles_AA)):\n",
    "\n",
    "    axs17[i//3,i%3].plot(Roles_AA[i], Correlations_AA[i], 'o')\n",
    "    axs17[i//3,i%3].set_title(filenames_AA[i])\n",
    "\n",
    "plt.close(fig17)\n",
    "fig17.tight_layout()\n",
    "fig17.savefig(join(plots_path,\"Correlation by role AA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c0d8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roles_CA, Correlations_CA, p_values_CA = [],[], []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "\n",
    "    filename = filenames_CA[i]\n",
    "    \n",
    "    roles, correlations, p_values = compute_corr_roles(data, \"Roles\", \"Speech rate\", \"Speaker\", \"Global_start\")\n",
    "    \n",
    "    Roles_CA.append(roles)\n",
    "    Correlations_CA.append(correlations)\n",
    "    p_values_CA.append(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cac0f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig18, axs18 = plt.subplots(3,3, figsize=(10,10))\n",
    "\n",
    "\n",
    "for i in range(len(Roles_CA)):\n",
    "\n",
    "    axs18[i//3,i%3].plot(Roles_CA[i], Correlations_CA[i], 'o')\n",
    "    axs18[i//3,i%3].set_title(filenames_CA[i])\n",
    "    \n",
    "plt.close(fig18)\n",
    "fig18.tight_layout()\n",
    "fig18.savefig(join(plots_path,\"Correlation by role CA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6b1b5",
   "metadata": {},
   "source": [
    "# Number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00165c1a",
   "metadata": {},
   "source": [
    "## Adult/adult vs Adult/child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48f451fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words_adult_1 = []\n",
    "nb_words_adult_2 = []\n",
    "\n",
    "\n",
    "for i, data in enumerate(data_AA) :\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Adult1\"]\n",
    "    data_2 = data[data[\"Speaker\"]==\"Adult2\"]\n",
    "    \n",
    "    mean_1 = np.mean(data_1[\"Word\"])\n",
    "    mean_2 = np.mean(data_2[\"Word\"])\n",
    "    \n",
    "    nb_words_adult_1.append(mean_1)\n",
    "    nb_words_adult_2.append(mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65ecde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words_adult = []\n",
    "nb_words_child = []\n",
    "\n",
    "\n",
    "for i, data in enumerate(data_CA) :\n",
    "    \n",
    "    \n",
    "    data_adult = data[data[\"Speaker\"]==\"Parent\"]\n",
    "    data_child = data[data[\"Speaker\"]==\"Child\"]\n",
    "        \n",
    "    mean_adult = np.mean(data_adult[\"Word\"])\n",
    "    mean_child = np.mean(data_child[\"Word\"])\n",
    "    \n",
    "    nb_words_adult.append(mean_adult)\n",
    "    nb_words_child.append(mean_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8051b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig19, axs19 = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "    \n",
    "    axs19[0].plot([\"Adult 1\", \"Adult 2\"], [nb_words_adult_1[i], nb_words_adult_2[i]], 'o', label=filename)\n",
    "    \n",
    "axs19[0].axhline(y=np.mean(nb_words_adult_1), label=\"Adult1\", color='r')\n",
    "axs19[0].axhline(y=np.mean(nb_words_adult_2), label=\"Adult2\", color='b')\n",
    "\n",
    "for j, filename in enumerate(filenames_CA):\n",
    "    \n",
    "    axs19[1].plot([\"Parent\", \"Child\"], [nb_words_adult[j], nb_words_child[j]], 'o', label=filename)\n",
    "    \n",
    "axs19[1].axhline(y=np.mean(nb_words_adult), label=\"Adult\", color='r')\n",
    "axs19[1].axhline(y=np.mean(nb_words_child), label=\"Child\", color='b')\n",
    "    \n",
    "axs19[0].set_title(\"Mean number of words for both adults\")\n",
    "axs19[1].set_title(\"Mean number of words for adult and child\")\n",
    "\n",
    "axs19[0].legend(loc=9,fontsize=8)\n",
    "axs19[1].legend(loc=9,fontsize=8)\n",
    "\n",
    "plt.close(fig19)\n",
    "fig19.savefig(join(plots_path,\"Mean number of words AA VS CA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae6d25",
   "metadata": {},
   "source": [
    "## Coding roles in a more convenient way (ask or answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13b2a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    roles_file = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        if (data[\"Speaker\"][j]==\"Adult1\" and data[\"Roles\"][j]==\"SPEAKER1 \") or (data[\"Speaker\"][j]==\"Adult2\" and data[\"Roles\"][j]==\"SPEAKER2 \"):\n",
    "            roles_file.append(\"ans\")\n",
    "            \n",
    "        else:\n",
    "            roles_file.append(\"ask\")\n",
    "            \n",
    "    data[\"New_role\"]=roles_file\n",
    "    adapted_data_AA[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "485d1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    roles_file = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        if (data[\"Speaker\"][j]==\"Parent\" and data[\"Roles\"][j]==\"SPEAKER1 \") or (data[\"Speaker\"][j]==\"Child\" and data[\"Roles\"][j]==\"SPEAKER2 \"):\n",
    "            roles_file.append(\"ans\")\n",
    "            \n",
    "        else:\n",
    "            roles_file.append(\"ask\")\n",
    "            \n",
    "    data[\"New_role\"]=roles_file\n",
    "    adapted_data_CA[i] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcebb8",
   "metadata": {},
   "source": [
    "## Number of words by role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8542d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_1_ans = []\n",
    "adult_1_ask = []\n",
    "\n",
    "adult_2_ans = []\n",
    "adult_2_ask = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Adult1\"]\n",
    "    \n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Word\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Word\"])\n",
    "    \n",
    "    adult_1_ans.append(mean_1_ans)\n",
    "    adult_1_ask.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Adult2\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Word\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Word\"])\n",
    "    \n",
    "    adult_2_ans.append(mean_2_ans)\n",
    "    adult_2_ask.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7910b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_ans = []\n",
    "adult_ask = []\n",
    "\n",
    "child_ans = []\n",
    "child_ask = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Parent\"]\n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Word\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Word\"])\n",
    "    \n",
    "    adult_ans.append(mean_1_ans)\n",
    "    adult_ask.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Child\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Word\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Word\"])\n",
    "    \n",
    "    child_ans.append(mean_2_ans)\n",
    "    child_ask.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9a6fbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 503.75x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "speaker = [\"Adult 1\" for i in range(len(adult_1_ans)+len(adult_1_ask))]+[\"Adult 2\" for i in range(len(adult_2_ans)+len(adult_2_ask))]\n",
    "role = [\"Answer\" for i in range(len(adult_1_ans))] + [\"Ask\" for i in range(len(adult_1_ask))] + [\"Answer\" for i in range(len(adult_2_ans))]+ [\"Ask\" for i in range(len(adult_2_ask))]\n",
    "words = adult_1_ans + adult_1_ask + adult_2_ans + adult_2_ask\n",
    "dico_AA = {\"Speaker\":speaker, \"Role\":role,\"Words\":words}\n",
    "df_AA = pd.DataFrame(dico_AA)\n",
    "\n",
    "fig20 = sns.catplot(data=df_AA, kind=\"bar\", x=\"Role\", y=\"Words\", hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig20.despine(left=True)\n",
    "fig20.set_axis_labels(\"\", \"Mean number of words per utterance\")\n",
    "fig20.legend.set_title(\"\")\n",
    "\n",
    "fig20.savefig(join(plots_path,\"Mean nb of words by role AA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e4d140f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 494.25x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "speaker = [\"Adult\" for i in range(len(adult_ans)+len(adult_ask))]+[\"Child\" for i in range(len(child_ans)+len(child_ask))]\n",
    "role = [\"Answer\" for i in range(len(adult_ans))] + [\"Ask\" for i in range(len(adult_ask))] + [\"Answer\" for i in range(len(child_ans))]+ [\"Ask\" for i in range(len(child_ask))]\n",
    "words = adult_ans + adult_ask + child_ans + child_ask\n",
    "dico_CA = {\"Speaker\":speaker, \"Role\":role,\"Words\":words}\n",
    "df_CA = pd.DataFrame(dico_CA)\n",
    "\n",
    "fig21 = sns.catplot(data=df_CA, kind=\"bar\", x=\"Role\", y=\"Words\",hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig21.despine(left=True)\n",
    "fig21.set_axis_labels(\"\", \"Mean number of words per utterance\")\n",
    "fig21.legend.set_title(\"\")\n",
    "\n",
    "fig21.savefig(join(plots_path,\"Mean nb of words by role CA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c1396",
   "metadata": {},
   "source": [
    "# Mean speech rate by role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31250483",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_1_ans_SR = []\n",
    "adult_1_ask_SR = []\n",
    "\n",
    "adult_2_ans_SR = []\n",
    "adult_2_ask_SR = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Adult1\"]\n",
    "    \n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Speech rate\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Speech rate\"])\n",
    "    \n",
    "    adult_1_ans_SR.append(mean_1_ans)\n",
    "    adult_1_ask_SR.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Adult2\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Speech rate\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Speech rate\"])\n",
    "    \n",
    "    adult_2_ans_SR.append(mean_2_ans)\n",
    "    adult_2_ask_SR.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66b203a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_ans_SR = []\n",
    "adult_ask_SR = []\n",
    "\n",
    "child_ans_SR = []\n",
    "child_ask_SR = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Parent\"]\n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Speech rate\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Speech rate\"])\n",
    "    \n",
    "    adult_ans_SR.append(mean_1_ans)\n",
    "    adult_ask_SR.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Child\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Speech rate\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Speech rate\"])\n",
    "    \n",
    "    child_ans_SR.append(mean_2_ans)\n",
    "    child_ask_SR.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57150a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 503.75x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "speaker = [\"Adult 1\" for i in range(len(adult_1_ans_SR)+len(adult_1_ask_SR))]+[\"Adult 2\" for i in range(len(adult_2_ans_SR)+len(adult_2_ask_SR))]\n",
    "role = [\"Answer\" for i in range(len(adult_1_ans_SR))] + [\"Ask\" for i in range(len(adult_1_ask_SR))] + [\"Answer\" for i in range(len(adult_2_ans_SR))]+ [\"Ask\" for i in range(len(adult_2_ask_SR))]\n",
    "SR = adult_1_ans_SR + adult_1_ask_SR + adult_2_ans_SR + adult_2_ask_SR\n",
    "dico_AA = {\"Speaker\":speaker, \"Role\":role,\"SR\":SR}\n",
    "df_AA = pd.DataFrame(dico_AA)\n",
    "\n",
    "fig22 = sns.catplot(data=df_AA, kind=\"bar\", x=\"Role\", y=\"SR\", hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig22.despine(left=True)\n",
    "fig22.set_axis_labels(\"\", \"Mean speech rate\")\n",
    "fig22.legend.set_title(\"\")\n",
    "\n",
    "fig22.savefig(join(plots_path,\"Mean SR by role AA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d25d4cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 494.25x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "speaker = [\"Adult\" for i in range(len(adult_ans_SR)+len(adult_ask_SR))]+[\"Child\" for i in range(len(child_ans_SR)+len(child_ask_SR))]\n",
    "role = [\"Answer\" for i in range(len(adult_ans_SR))] + [\"Ask\" for i in range(len(adult_ask_SR))] + [\"Answer\" for i in range(len(child_ans_SR))]+ [\"Ask\" for i in range(len(child_ask_SR))]\n",
    "SR = adult_ans_SR + adult_ask_SR + child_ans_SR + child_ask_SR\n",
    "dico_CA = {\"Speaker\":speaker, \"Role\":role,\"SR\":SR}\n",
    "df_CA = pd.DataFrame(dico_CA)\n",
    "\n",
    "fig23 = sns.catplot(data=df_CA, kind=\"bar\", x=\"Role\", y=\"SR\",hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig23.despine(left=True)\n",
    "fig23.set_axis_labels(\"\", \"Mean SR per utterance\")\n",
    "fig23.legend.set_title(\"\")\n",
    "\n",
    "fig23.savefig(join(plots_path,\"Mean SR by role CA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed2bbd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK !\n"
     ]
    }
   ],
   "source": [
    "print(\"OK !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183f933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
