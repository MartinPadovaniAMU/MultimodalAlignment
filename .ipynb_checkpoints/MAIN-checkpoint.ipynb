{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e382b0b",
   "metadata": {},
   "source": [
    "# Importing modules and lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a636d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random as random\n",
    "import re\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.signal import savgol_filter\n",
    "import scipy.spatial.distance as ssd\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "from MY_FUNCTIONS import *\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18e2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = pd.read_csv('C:/Users/Ignifuge/Downloads/Lexique383.tsv', sep='\\t')\n",
    "lexicon = lexicon[['ortho', 'nbsyll']]\n",
    "dictionnary = dict(zip(lexicon['ortho'], lexicon ['nbsyll']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f4cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_path = \"C:/Users/Ignifuge/Downloads/Corrected transcriptions/plots/\"\n",
    "path = = \"C:/Users/Ignifuge/Downloads/Corrected transcriptions/\"\n",
    "#directory to stock all the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cf160",
   "metadata": {},
   "source": [
    "# Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad0831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_silences = 0  #silences between words of a same utterance\n",
    "remove_shorter = 1  #utterances with only one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964d3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_red = (0.8,0,0)\n",
    "my_blue= (0,0.4,0.8)\n",
    "my_pink = (1,0.4,0.4)\n",
    "my_green = (0,0.8,0.4)\n",
    "my_purple = (0.6,0,0.3)\n",
    "my_yellow = (0.89, 0.75, 0)\n",
    "my_orange = (0.82, 0.55, 0)\n",
    "my_turquoise = (0, 0.72, 0.58)\n",
    "my_bordeaux = (0.51, 0.02, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4565a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colors = [my_red, my_blue, my_pink, my_green, my_purple, my_yellow, my_orange, my_turquoise, my_bordeaux]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac6fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_window = 9\n",
    "default_poly = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d06cb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_AA = 'C:/Users/Ignifuge/Downloads/Corrected transcriptions/AA/'\n",
    "path_CA = 'C:/Users/Ignifuge/Downloads/Corrected transcriptions/CA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c46181",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35404661",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = pd.read_csv(\"word.csv\")\n",
    "WORDS = WORDS[WORDS['Length']!=0] #to keep only non-zero intervals\n",
    "filenames = WORDS['Filename'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6827df",
   "metadata": {},
   "source": [
    "# Separating AA and CA files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18bd23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_AA = []\n",
    "WORDS_CA = []\n",
    "for filename in filenames : \n",
    "    WORDS_file = WORDS[ WORDS['Filename']==filename][['Word','UtteranceName','Speaker', 'Length', 'Global_start', 'Filename', 'Global_end']]\n",
    "    WORDS_file.reset_index(drop=True, inplace=True)\n",
    "    WORDS_file=WORDS_file.sort_values('Speaker', ascending=True)\n",
    "    if filename[0:2]=='AA':\n",
    "        WORDS_AA.append(WORDS_file)\n",
    "    else:\n",
    "        WORDS_CA.append(WORDS_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b16e6",
   "metadata": {},
   "source": [
    "# Creating a csv file for each conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b7d5c",
   "metadata": {},
   "source": [
    "## Computing the \"syllables\" column, and grouping words by utterances (keeping the first start and last end, summing syllables and length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51f6e5",
   "metadata": {},
   "source": [
    "for i,WORDS in enumerate(WORDS_AA) :\n",
    "        \n",
    "    list_syll = []\n",
    "    for word in WORDS['Word']:\n",
    "        word=word.lower()\n",
    "        if word in dictionnary:\n",
    "            list_syll.append(dictionnary[word])\n",
    "        else : \n",
    "            list_syll.append(estimate_syll(word))\n",
    "            \n",
    "    WORDS[\"syllables\"] = list_syll\n",
    "    WORDS=WORDS.sort_values('Global_start', ascending=True)\n",
    "    WORDS_AA[i] = WORDS\n",
    "        \n",
    "    table = pd.pivot_table(WORDS, values = ['Length', 'Global_start', 'Global_end','syllables', 'Speaker', 'Filename', 'Word'], index = ['UtteranceName'], aggfunc = {'Length':np.sum, 'Global_start':min, 'syllables':np.sum, 'Speaker' : np.min, 'Filename':np.min, 'Word':'count', 'Global_end':max})\n",
    "    table = table.sort_values('Global_start', ascending = True)\n",
    "    if remove_silences:\n",
    "        table['Speech rate'] = table['syllables']/table['Length']\n",
    "    else:\n",
    "        table['Speech rate'] = table['syllables']/(table['Global_end']-table['Global_start'])\n",
    "    if remove_shorter:\n",
    "        table = table[ table['Word']>1]\n",
    "    WORDS_AA[i] = table\n",
    "    table.to_csv(path_AA+str(WORDS['Filename'][0])+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3d76e",
   "metadata": {},
   "source": [
    "for i,WORDS in enumerate(WORDS_CA) : \n",
    "        \n",
    "    list_syll = []\n",
    "    for word in WORDS['Word']:\n",
    "        word=str(word)\n",
    "        word = word.lower()\n",
    "        if word in dictionnary:\n",
    "            list_syll.append(dictionnary[word])\n",
    "        else : \n",
    "            list_syll.append(estimate_syll(word))\n",
    "    WORDS[\"syllables\"] = list_syll\n",
    "    WORDS_CA[i] = WORDS\n",
    "    \n",
    "    WORDS['Word'] = np.vectorize(str)(WORDS['Word'])\n",
    "    WORDS.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    table = pd.pivot_table(WORDS, values = ['Word', 'UtteranceName', 'Global_start', 'Global_end', 'Length', \"Filename\", \"syllables\"], index = ['UtteranceName', \"Speaker\"], aggfunc = {\"Global_end\":max,\"UtteranceName\":np.unique,'Word':'count', 'Global_start':min, \"Length\":sum, \"Speaker\":np.unique, \"Filename\":np.unique, \"syllables\":sum})    \n",
    "    table = table.sort_values('Global_start', ascending = True)\n",
    "    \n",
    "    if remove_silences :\n",
    "        table['Speech rate'] = table['syllables']/table['Length']\n",
    "    else : \n",
    "        table['Speech rate'] = table['syllables']/(table['Global_end']-table['Global_start'])\n",
    "    if remove_shorter:\n",
    "        table = table[ table['Word']>1]\n",
    "    WORDS_CA[i] = table\n",
    "    \n",
    "    table.to_csv(path_CA+str(WORDS['Filename'][0])+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3b1e1",
   "metadata": {},
   "source": [
    "# Importing the new csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af7b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-AN-DL.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-GD-DD.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-JL-AZ.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-LA-AN.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-LD-BF.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-MG-CH.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-MJ-CJ.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-ML-MP.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-XA-EH.csv\n"
     ]
    }
   ],
   "source": [
    "pathlist_AA = Path(path_AA).glob('**/*.csv')\n",
    "data_AA = []\n",
    "filenames_AA = []\n",
    "\n",
    "for path in pathlist_AA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        filenames_AA.append(str(path)[-12:-4])\n",
    "        data_AA.append(pd.read_csv(path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc9cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-AN-ZN.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-FB-MG.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-JL-JT.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-LD-GD.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-LJ-MJ.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-MB-LB.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-MD-GD.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-RL-ML.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-XA-LA.csv\n"
     ]
    }
   ],
   "source": [
    "pathlist_CA = Path(path_CA).glob('**/*.csv')\n",
    "data_CA = []\n",
    "filenames_CA = []\n",
    "\n",
    "for path in pathlist_CA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        filenames_CA.append(str(path)[-12:-4])\n",
    "        data_CA.append(pd.read_csv(path)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc093d",
   "metadata": {},
   "source": [
    "# Raw speech rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4563a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axs1 = plt.subplots(3, 3, figsize=(20,17))\n",
    "\n",
    "for i, data in enumerate(data_AA) :\n",
    "    \n",
    "    data_1 = data[ data['Speaker'] == \"Adult1\" ]\n",
    "    data_2 = data[ data['Speaker'] == \"Adult2\" ]\n",
    "    \n",
    "    axs1[i//3,i%3].set_title(data['Filename'][0])\n",
    "    axs1[i//3,i%3].set_xlabel('Time (sec)')\n",
    "    axs1[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs1[i//3,i%3].plot(data_1['Global_start'], data_1['Speech rate'], label=\"Adult1\", color=my_blue)\n",
    "    axs1[i//3,i%3].plot(data_2['Global_start'], data_2['Speech rate'], label=\"Adult2\", color=my_red)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig1)\n",
    "fig1.savefig(plots_path+\"Raw AA speech rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad34c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axs2 = plt.subplots(3, 3, figsize=(20,17))\n",
    "\n",
    "for i, data in enumerate(data_CA) :\n",
    "    \n",
    "    data_1 = data[ data['Speaker'] == \"Parent\" ]\n",
    "    data_2 = data[ data['Speaker'] == \"Child\" ]\n",
    "    \n",
    "    axs2[i//3,i%3].set_title(data['Filename'][0])\n",
    "    axs2[i//3,i%3].set_xlabel('Time (sec)')\n",
    "    axs2[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs2[i//3,i%3].plot(data_1['Global_start'], data_1['Speech rate'], label=\"Parent\", color=my_blue)\n",
    "    axs2[i//3,i%3].plot(data_2['Global_start'], data_2['Speech rate'], label=\"Child\", color=my_red)\n",
    "    \n",
    "fig2.tight_layout()\n",
    "plt.close(fig2)\n",
    "fig2.savefig(plots_path+\"Raw CA speech rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6df9c",
   "metadata": {},
   "source": [
    "# Fourier Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f89dca",
   "metadata": {},
   "source": [
    "# Smoothed speech rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84840082",
   "metadata": {},
   "source": [
    "## With a smoothing window of 10 utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b46870",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axs3 = plt.subplots(3, 3, figsize=(20,17))\n",
    "\n",
    "for i, table in enumerate(data_AA) :\n",
    "    \n",
    "    axs3[i//3,i%3].set_xlabel('time')\n",
    "    axs3[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs3[i//3,i%3].plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], 29, 3), label='Adult1',color = my_red)\n",
    "    axs3[i//3,i%3].plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], 29, 3), label='Adult2', color=my_blue)\n",
    "    axs3[i//3,i%3].set_title(str(table['Filename'][0]))\n",
    "    axs3[i//3,i%3].legend()\n",
    "    \n",
    "    if \"AA-LD-BF\" in filenames_AA[i]:\n",
    "        fig35 = plt.figure()\n",
    "        plt.plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], 29, 3), label='Adult1',color = my_red)\n",
    "        plt.plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], 29, 3), label='Adult2', color=my_blue)\n",
    "        plt.xlabel(\"time (s)\")\n",
    "        plt.ylabel(\"Speech rate (syllables/s)\")\n",
    "        plt.close(fig35)\n",
    "        fig35.savefig(plots_path+\"Smoothed AA\")\n",
    "    \n",
    "fig3.tight_layout()\n",
    "plt.close(fig3)\n",
    "fig3.savefig(plots_path+\"Smoothed AA speech rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39380d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4, axs4 = plt.subplots(3, 3, figsize=(20,15))\n",
    "\n",
    "for i, table in enumerate(data_CA) :\n",
    "    \n",
    "    axs4[i//3,i%3].set_xlabel('time')\n",
    "    axs4[i//3,i%3].set_ylabel('Speech rate')\n",
    "    axs4[i//3,i%3].plot(table[table['Speaker']=='Parent']['Global_start'], smooth_2(table[table['Speaker']=='Parent']['Speech rate'],29, 3), label='Parent',color = my_red)\n",
    "    axs4[i//3,i%3].plot( table[table['Speaker']=='Child']['Global_start'], smooth_2(table[table['Speaker']=='Child']['Speech rate'], 29, 3), label='Child', color=my_blue)\n",
    "    axs4[i//3,i%3].set_title(str(table['Filename'][0]))\n",
    "    axs4[i//3,i%3].legend()\n",
    "    \n",
    "    if \"CA-LD-GD\" in filenames_CA[i]:\n",
    "        fig45 = plt.figure()\n",
    "        plt.plot(table[table['Speaker']=='Parent']['Global_start'], smooth_2(table[table['Speaker']=='Parent']['Speech rate'], 29, 3), label='Parent',color = my_red)\n",
    "        plt.plot( table[table['Speaker']=='Child']['Global_start'], smooth_2(table[table['Speaker']=='Child']['Speech rate'], 29, 3), label='Child', color=my_blue)\n",
    "        plt.xlabel(\"time (s)\")\n",
    "        plt.ylabel(\"Speech rate (syllables/s)\")\n",
    "        plt.close(fig45)\n",
    "        fig45.savefig(plots_path+\"Smoothed CA\")\n",
    "    \n",
    "fig4.tight_layout()\n",
    "plt.close(fig4)\n",
    "fig4.savefig(plots_path+\"Smoothed CA speech rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5286b5",
   "metadata": {},
   "source": [
    "# \"Adapted\" speech rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be785b00",
   "metadata": {},
   "source": [
    "## To compute the correlation, the time series must have the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "807caaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "listes_roles = []\n",
    "for i, data in enumerate(data_CA):\n",
    "    liste_role = []\n",
    "    for i in range(len(data)):\n",
    "        liste_role.append((data['Speaker'][i], data['Global_start'][i]))\n",
    "    listes_roles.append(liste_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e61d08",
   "metadata": {},
   "source": [
    "### Loading manual phases notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea396e00",
   "metadata": {},
   "source": [
    "text_AA = open(path_AA + \"annotations.txt\", 'r').read()\n",
    "\n",
    "lines_AA = text_AA.split('\\n')\n",
    "Filenames_AA = []\n",
    "Phases_AA = []\n",
    "line = lines_AA[0]\n",
    "file = []\n",
    "for line in lines_AA:\n",
    "    if line!='' and line!=' ':\n",
    "        if \"END\" not in line :\n",
    "            if \"FILENAME\" not in line:\n",
    "                label, time_string = line.split('[')\n",
    "                time_string = time_string.replace('[', '')\n",
    "                time_string = time_string.replace(']', '')\n",
    "                time_string = time_string.replace(' ', '')\n",
    "                time = conversion_time(time_string)\n",
    "                file.append([label, time])\n",
    "            else:\n",
    "                NOM, filename = line.split(' ')[1:]\n",
    "        else:\n",
    "            Filenames_AA.append(filename)\n",
    "            Phases_AA.append(file)\n",
    "            file=[]\n",
    "phases_AA = pd.DataFrame(list(zip(Filenames_AA, Phases_AA)), columns = ['Filename', 'Phases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329eb20",
   "metadata": {},
   "source": [
    "text_CA = open(path_CA + \"annotations.txt\", 'r').read()\n",
    "\n",
    "lines_CA = text_CA.split('\\n')\n",
    "Filenames_CA = []\n",
    "Phases_CA = []\n",
    "line = lines_CA[0]\n",
    "file = []\n",
    "for line in lines_CA:\n",
    "    if line!='' and line!=' ':\n",
    "        if \"END\" not in line :\n",
    "            if \"FILENAME\" not in line:\n",
    "                label, time_string = line.split('[')\n",
    "                time_string = time_string.replace('[', '')\n",
    "                time_string = time_string.replace(']', '')\n",
    "                time_string = time_string.replace(' ', '')\n",
    "                time = conversion_time(time_string)\n",
    "                file.append([label, time])\n",
    "            else:\n",
    "                NOM, filename = line.split(' ')[1:]\n",
    "        else:\n",
    "            Filenames_CA.append(filename)\n",
    "            Phases_CA.append(file)\n",
    "            file=[]\n",
    "phases_CA = pd.DataFrame(list(zip(Filenames_CA, Phases_CA)), columns = ['Filename', 'Phases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25465f43",
   "metadata": {},
   "source": [
    "### Adding phases, roles and smoothed columns, save as csv, and adapt the number of points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf3074",
   "metadata": {},
   "source": [
    "for i,filename in enumerate(filenames_AA):\n",
    "    \n",
    "    print(filename)\n",
    "    \n",
    "    data = data_AA[i]\n",
    "    \n",
    "    adapted_data = adapt_points_3(data,\"Speaker\", \"Global_start\", \"Speech rate\")\n",
    "    \n",
    "    liste_phases = list(phases_AA[phases_AA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    adapted_data['Phases'] = find_phases(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data['Roles'] = find_roles(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data[\"Word\"] = data[\"Word\"]\n",
    "    \n",
    "    adapted_data[\"smoothed\"] = smoothed_column(adapted_data, default_window, 'AA')\n",
    "    \n",
    "    adapted_data.to_csv(path_AA+str(filename)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ec97b",
   "metadata": {},
   "source": [
    "for i,filename in enumerate(filenames_CA):\n",
    "    \n",
    "    print(filename)\n",
    "    \n",
    "    data = data_CA[i]\n",
    "    \n",
    "    adapted_data = adapt_points_3(data,\"Speaker\", \"Global_start\", \"Speech rate\")\n",
    "    \n",
    "    liste_phases = list(phases_CA[phases_CA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    adapted_data['Phases'] = find_phases(adapted_data['Global_start'], liste_phases)\n",
    "    adapted_data['Roles'] = find_roles(adapted_data['Global_start'], listes_roles[i])\n",
    "    adapted_data[\"Word\"] = data[\"Word\"]\n",
    "    \n",
    "    adapted_data[\"smoothed\"] = smoothed_column(adapted_data, default_window, 'CA')\n",
    "        \n",
    "    adapted_data.to_csv(path_CA+str(filename)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c59e65",
   "metadata": {},
   "source": [
    "### Importing the new csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ae507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-AN-DL.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-GD-DD.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-JL-AZ.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-LA-AN.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-LD-BF.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-MG-CH.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-MJ-CJ.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-ML-MP.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\AA\\AA-XA-EH.csv\n",
      "[           UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult1_AA-AN-DL  AA-AN-DL       27.96          9.16   10.05  Adult1   \n",
      "1    111_Adult2_AA-AN-DL  AA-AN-DL       29.99         29.59    0.40  Adult2   \n",
      "2      1_Adult1_AA-AN-DL  AA-AN-DL       31.27         30.06    1.08  Adult1   \n",
      "3    112_Adult2_AA-AN-DL  AA-AN-DL       36.88         32.16    1.25  Adult2   \n",
      "4      2_Adult1_AA-AN-DL  AA-AN-DL       37.16         34.11    1.32  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "168  108_Adult1_AA-AN-DL  AA-AN-DL      956.42        951.20    2.70  Adult1   \n",
      "169  220_Adult2_AA-AN-DL  AA-AN-DL      965.00        961.00    3.12  Adult2   \n",
      "170  221_Adult2_AA-AN-DL  AA-AN-DL      965.21        965.00    0.21  Adult2   \n",
      "171  110_Adult1_AA-AN-DL  AA-AN-DL      966.97        966.29    0.68  Adult1   \n",
      "172  222_Adult2_AA-AN-DL  AA-AN-DL      970.16        967.52    1.70  Adult2   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0      83        107     5.691489  \n",
      "1       2          3     7.500000  \n",
      "2       7          8     6.611570  \n",
      "3       6          7     1.483051  \n",
      "4       4          4     1.311475  \n",
      "..    ...        ...          ...  \n",
      "168    13         16     3.065134  \n",
      "169    30         43    10.750000  \n",
      "170     2          2     9.523810  \n",
      "171     5          7    10.294118  \n",
      "172     5          7     2.651515  \n",
      "\n",
      "[173 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult2_AA-GD-DD  AA-GD-DD        4.04          2.46    1.58  Adult2   \n",
      "1     80_Adult1_AA-GD-DD  AA-GD-DD       16.97          4.52    3.33  Adult1   \n",
      "2      1_Adult2_AA-GD-DD  AA-GD-DD       51.09          8.56    5.61  Adult2   \n",
      "3     81_Adult1_AA-GD-DD  AA-GD-DD       53.72         18.21   17.49  Adult1   \n",
      "4      2_Adult2_AA-GD-DD  AA-GD-DD       54.67         54.29    0.38  Adult2   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "133  157_Adult1_AA-GD-DD  AA-GD-DD      909.90        901.32    1.88  Adult1   \n",
      "134   78_Adult2_AA-GD-DD  AA-GD-DD      922.48        902.91   12.09  Adult2   \n",
      "135  158_Adult1_AA-GD-DD  AA-GD-DD      932.00        924.74    3.59  Adult1   \n",
      "136   79_Adult2_AA-GD-DD  AA-GD-DD      938.66        927.10    2.59  Adult2   \n",
      "137  159_Adult1_AA-GD-DD  AA-GD-DD      944.00        936.74    3.59  Adult1   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0       7          9     5.696203  \n",
      "1      15         18     1.445783  \n",
      "2      30         33     0.775923  \n",
      "3      86        108     3.041397  \n",
      "4       2          3     7.894737  \n",
      "..    ...        ...          ...  \n",
      "133    10         15     1.748252  \n",
      "134    58         81     4.138988  \n",
      "135    14         22     3.030303  \n",
      "136     9         12     1.038062  \n",
      "137    14         22     3.030303  \n",
      "\n",
      "[138 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult2_AA-JL-AZ  AA-JL-AZ        4.80          1.96    1.98  Adult2   \n",
      "1    147_Adult1_AA-JL-AZ  AA-JL-AZ        5.85          4.61    1.09  Adult1   \n",
      "2      1_Adult2_AA-JL-AZ  AA-JL-AZ        6.91          6.62    0.29  Adult2   \n",
      "3      2_Adult2_AA-JL-AZ  AA-JL-AZ       13.83          8.43    3.36  Adult2   \n",
      "4    150_Adult1_AA-JL-AZ  AA-JL-AZ       18.50         16.49    1.58  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "204  287_Adult1_AA-JL-AZ  AA-JL-AZ      937.74        937.10    0.64  Adult1   \n",
      "205  145_Adult2_AA-JL-AZ  AA-JL-AZ      948.68        940.66    2.42  Adult2   \n",
      "206  288_Adult1_AA-JL-AZ  AA-JL-AZ      949.00        948.68    0.32  Adult1   \n",
      "207  289_Adult1_AA-JL-AZ  AA-JL-AZ      953.72        949.00    1.31  Adult1   \n",
      "208  146_Adult2_AA-JL-AZ  AA-JL-AZ      953.87        952.06    0.67  Adult2   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0      10         16     5.633803  \n",
      "1       8          8     6.451613  \n",
      "2       2          4    13.793103  \n",
      "3      15         21     3.888889  \n",
      "4      10         13     6.467662  \n",
      "..    ...        ...          ...  \n",
      "204     2          2     3.125000  \n",
      "205     3          4     0.498753  \n",
      "206     2          2     6.250000  \n",
      "207     9         13     2.754237  \n",
      "208    10         11     6.077348  \n",
      "\n",
      "[209 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult2_AA-LA-AN  AA-LA-AN       6.790          1.90   0.730  Adult2   \n",
      "1     72_Adult1_AA-LA-AN  AA-LA-AN      15.000         11.57   1.340  Adult1   \n",
      "2      1_Adult2_AA-LA-AN  AA-LA-AN      16.510         14.00   0.480  Adult2   \n",
      "3     74_Adult1_AA-LA-AN  AA-LA-AN      28.700         20.53   6.940  Adult1   \n",
      "4      3_Adult2_AA-LA-AN  AA-LA-AN      30.000         29.48   0.520  Adult2   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "103  135_Adult1_AA-LA-AN  AA-LA-AN     936.730        919.70   3.680  Adult1   \n",
      "104  136_Adult1_AA-LA-AN  AA-LA-AN     980.740        955.08  13.550  Adult1   \n",
      "105   70_Adult2_AA-LA-AN  AA-LA-AN     986.000        960.00   7.480  Adult2   \n",
      "106  137_Adult1_AA-LA-AN  AA-LA-AN     996.704        982.35   4.174  Adult1   \n",
      "107   71_Adult2_AA-LA-AN  AA-LA-AN     996.220        986.12   1.960  Adult2   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0       3          4     0.817996  \n",
      "1       4          5     1.457726  \n",
      "2       2          2     0.796813  \n",
      "3      25         31     3.794370  \n",
      "4       2          3     5.769231  \n",
      "..    ...        ...          ...  \n",
      "103     5          9     0.528479  \n",
      "104    46         62     2.416212  \n",
      "105    62         96     3.692308  \n",
      "106     9          9     0.627003  \n",
      "107    23         28     2.772277  \n",
      "\n",
      "[108 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult1_AA-LD-BF  AA-LD-BF        0.68          0.00    0.30  Adult1   \n",
      "1      1_Adult1_AA-LD-BF  AA-LD-BF        8.97          6.03    2.61  Adult1   \n",
      "2      2_Adult1_AA-LD-BF  AA-LD-BF       15.86         12.00    2.61  Adult1   \n",
      "3    154_Adult2_AA-LD-BF  AA-LD-BF       17.35         16.58    0.70  Adult2   \n",
      "4      3_Adult1_AA-LD-BF  AA-LD-BF       27.75         17.66    7.94  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "228  297_Adult2_AA-LD-BF  AA-LD-BF     1022.41       1012.27    3.16  Adult2   \n",
      "229  149_Adult1_AA-LD-BF  AA-LD-BF     1020.80       1020.26    0.18  Adult1   \n",
      "230  150_Adult1_AA-LD-BF  AA-LD-BF     1028.94       1021.51    3.19  Adult1   \n",
      "231  151_Adult1_AA-LD-BF  AA-LD-BF     1030.37       1029.00    1.19  Adult1   \n",
      "232  299_Adult2_AA-LD-BF  AA-LD-BF     1033.56       1032.00    1.56  Adult2   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0       2          2     2.941176  \n",
      "1      12         13     4.421769  \n",
      "2      11         17     4.404145  \n",
      "3       2          2     2.597403  \n",
      "4      40         51     5.054509  \n",
      "..    ...        ...          ...  \n",
      "228    21         30     2.958580  \n",
      "229     2          2     3.703704  \n",
      "230    12         16     2.153432  \n",
      "231     8         12     8.759124  \n",
      "232    10         16    10.256410  \n",
      "\n",
      "[233 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult1_AA-MG-CH  AA-MG-CH       12.60          2.35    2.42  Adult1   \n",
      "1     78_Adult2_AA-MG-CH  AA-MG-CH       15.92         13.40    1.82  Adult2   \n",
      "2      1_Adult1_AA-MG-CH  AA-MG-CH       23.02         15.87    2.82  Adult1   \n",
      "3     79_Adult2_AA-MG-CH  AA-MG-CH       41.41         23.64    1.21  Adult2   \n",
      "4      2_Adult1_AA-MG-CH  AA-MG-CH       49.14         26.00   13.74  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "124   75_Adult1_AA-MG-CH  AA-MG-CH      905.13        903.27    0.86  Adult1   \n",
      "125  154_Adult2_AA-MG-CH  AA-MG-CH      912.42        904.82    7.13  Adult2   \n",
      "126   76_Adult1_AA-MG-CH  AA-MG-CH      924.18        914.41    7.86  Adult1   \n",
      "127  155_Adult2_AA-MG-CH  AA-MG-CH      927.03        925.38    0.90  Adult2   \n",
      "128   77_Adult1_AA-MG-CH  AA-MG-CH      930.06        928.01    2.05  Adult1   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0      15         17     1.658537  \n",
      "1      12         12     4.761905  \n",
      "2       8         10     1.398601  \n",
      "3       5          5     0.281373  \n",
      "4      61         83     3.586863  \n",
      "..    ...        ...          ...  \n",
      "124     4          5     2.688172  \n",
      "125    39         48     6.315789  \n",
      "126    31         45     4.605937  \n",
      "127     6          8     4.848485  \n",
      "128    15         17     8.292683  \n",
      "\n",
      "[129 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult1_AA-MJ-CJ  AA-MJ-CJ       1.720          1.43   0.290  Adult1   \n",
      "1      1_Adult1_AA-MJ-CJ  AA-MJ-CJ       4.770          3.54   0.460  Adult1   \n",
      "2      2_Adult1_AA-MJ-CJ  AA-MJ-CJ      12.200          5.00   4.540  Adult1   \n",
      "3    119_Adult2_AA-MJ-CJ  AA-MJ-CJ      13.610         11.49   1.950  Adult2   \n",
      "4      3_Adult1_AA-MJ-CJ  AA-MJ-CJ      21.660         13.34   3.220  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "188  114_Adult1_AA-MJ-CJ  AA-MJ-CJ    1057.210       1051.60   4.910  Adult1   \n",
      "189  239_Adult2_AA-MJ-CJ  AA-MJ-CJ    1058.890       1056.36   1.730  Adult2   \n",
      "190  240_Adult2_AA-MJ-CJ  AA-MJ-CJ    1063.160       1060.78   2.380  Adult2   \n",
      "191  115_Adult1_AA-MJ-CJ  AA-MJ-CJ    1073.550       1067.63   3.440  Adult1   \n",
      "192  116_Adult1_AA-MJ-CJ  AA-MJ-CJ    1081.792       1074.36   4.392  Adult1   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0       2          2     6.896552  \n",
      "1       4          4     3.252033  \n",
      "2      22         28     3.888889  \n",
      "3       7          9     4.245283  \n",
      "4      22         28     3.365385  \n",
      "..    ...        ...          ...  \n",
      "188    27         32     5.704100  \n",
      "189     6         10     3.952569  \n",
      "190    10         14     5.882353  \n",
      "191    16         18     3.040541  \n",
      "192    28         42     5.651238  \n",
      "\n",
      "[193 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult1_AA-ML-MP  AA-ML-MP        2.26          0.00    1.87  Adult1   \n",
      "1     74_Adult2_AA-ML-MP  AA-ML-MP        2.92          2.35    0.49  Adult2   \n",
      "2      1_Adult1_AA-ML-MP  AA-ML-MP        4.34          3.23    1.11  Adult1   \n",
      "3     75_Adult2_AA-ML-MP  AA-ML-MP       11.64          4.84    4.26  Adult2   \n",
      "4      2_Adult1_AA-ML-MP  AA-ML-MP       19.91         12.39    4.26  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "125   71_Adult1_AA-ML-MP  AA-ML-MP      987.89        982.46    2.99  Adult1   \n",
      "126  148_Adult2_AA-ML-MP  AA-ML-MP      990.43        988.03    1.99  Adult2   \n",
      "127   72_Adult1_AA-ML-MP  AA-ML-MP     1011.63        991.61    5.29  Adult1   \n",
      "128  149_Adult2_AA-ML-MP  AA-ML-MP     1024.43        993.85   11.55  Adult2   \n",
      "129   73_Adult1_AA-ML-MP  AA-ML-MP     1037.25       1026.35    5.99  Adult1   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0       8         11     4.867257  \n",
      "1       2          2     3.508772  \n",
      "2       4          5     4.504505  \n",
      "3      25         31     4.558824  \n",
      "4      19         27     3.590426  \n",
      "..    ...        ...          ...  \n",
      "125    14         15     2.762431  \n",
      "126     9         11     4.583333  \n",
      "127    29         38     1.898102  \n",
      "128    40         53     1.733159  \n",
      "129    33         39     3.577982  \n",
      "\n",
      "[130 rows x 9 columns],            UtteranceName  Filename  Global_end  Global_start  Length Speaker  \\\n",
      "0      0_Adult1_AA-XA-EH  AA-XA-EH        1.52          0.00    0.91  Adult1   \n",
      "1    156_Adult2_AA-XA-EH  AA-XA-EH        3.01          2.39    0.51  Adult2   \n",
      "2      1_Adult1_AA-XA-EH  AA-XA-EH        8.47          5.07    2.24  Adult1   \n",
      "3    157_Adult2_AA-XA-EH  AA-XA-EH       17.10          8.38    0.72  Adult2   \n",
      "4      2_Adult1_AA-XA-EH  AA-XA-EH       33.37          9.57   11.13  Adult1   \n",
      "..                   ...       ...         ...           ...     ...     ...   \n",
      "257  309_Adult2_AA-XA-EH  AA-XA-EH     1223.50       1220.63    1.49  Adult2   \n",
      "258  153_Adult1_AA-XA-EH  AA-XA-EH     1221.10       1220.77    0.33  Adult1   \n",
      "259  154_Adult1_AA-XA-EH  AA-XA-EH     1225.56       1222.62    1.91  Adult1   \n",
      "260  310_Adult2_AA-XA-EH  AA-XA-EH     1236.87       1226.17    0.30  Adult2   \n",
      "261  155_Adult1_AA-XA-EH  AA-XA-EH     1238.22       1226.30    4.76  Adult1   \n",
      "\n",
      "     Word  syllables  Speech rate  \n",
      "0       6          7     4.605263  \n",
      "1       2          3     4.838710  \n",
      "2      13         17     5.000000  \n",
      "3       4          6     0.688073  \n",
      "4      77         99     4.159664  \n",
      "..    ...        ...          ...  \n",
      "257     8         10     3.484321  \n",
      "258     2          0     0.000000  \n",
      "259    10         13     4.421769  \n",
      "260     2          2     0.186916  \n",
      "261    23         32     2.684564  \n",
      "\n",
      "[262 rows x 9 columns]]\n"
     ]
    }
   ],
   "source": [
    "pathlist_AA = Path(path_AA).glob('**/*.csv')\n",
    "adapted_data_AA = []\n",
    "adapted_filenames_AA = []\n",
    "\n",
    "for path in pathlist_AA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        adapted_filenames_AA.append(str(path)[-12:-4])\n",
    "        adapted_data_AA.append(pd.read_csv(path))\n",
    "print(adapted_data_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f992e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-AN-ZN.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-FB-MG.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-JL-JT.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-LD-GD.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-LJ-MJ.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-MB-LB.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-MD-GD.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-RL-ML.csv\n",
      "C:\\Users\\Ignifuge\\Downloads\\Corrected transcriptions\\CA\\CA-XA-LA.csv\n"
     ]
    }
   ],
   "source": [
    "pathlist_CA = Path(path_CA).glob('**/*.csv')\n",
    "adapted_data_CA = []\n",
    "adapted_filenames_CA = []\n",
    "\n",
    "for path in pathlist_CA:\n",
    "    if \"BO\" not in str(path):\n",
    "        print(path)\n",
    "        adapted_filenames_CA.append(str(path)[-12:-4])\n",
    "        adapted_data_CA.append(pd.read_csv(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261c00b",
   "metadata": {},
   "source": [
    "### Plot of the adapted speech rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fdec7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5, axs5 = plt.subplots(9, 2, figsize=(15,20))\n",
    "\n",
    "for i, adapted_data in enumerate(adapted_data_AA) :\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Adult1\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Adult2\" ]  \n",
    "\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Adult1\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Adult2\" ]\n",
    "    \n",
    "    axs5[i,0].set_title(filename+\" speaker1\")\n",
    "    axs5[i,0].set_xlabel('Time (sec)')\n",
    "    axs5[i,0].set_ylabel('Speech rate')\n",
    "    axs5[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'], label=\"Raw\", color=my_blue)\n",
    "    axs5[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'],'o', label=\"Adapted\", color=my_red)\n",
    "    \n",
    "    axs5[i,1].set_title(filename+\" speaker2\")\n",
    "    axs5[i,1].set_xlabel('Time (sec)')\n",
    "    axs5[i,1].set_ylabel('Adapted speech rate')\n",
    "    axs5[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'],label=\"Raw\", color=my_blue)\n",
    "    axs5[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'], 'o',label=\"Adapted\", color=my_red)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig5)\n",
    "fig5.savefig(plots_path+\"Adapted AA speech rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e512bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6, axs6 = plt.subplots(9, 2, figsize=(15,20))\n",
    "\n",
    "for i, adapted_data in enumerate(adapted_data_CA) :\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Parent\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Child\" ]  \n",
    "\n",
    "    \n",
    "    adapted_data_1 = adapted_data[ adapted_data['Speaker'] == \"Parent\" ]\n",
    "    adapted_data_2 = adapted_data[ adapted_data['Speaker'] == \"Child\" ]\n",
    "    \n",
    "    axs6[i,0].set_title(filename+\" speaker1\")\n",
    "    axs6[i,0].set_xlabel('Time (sec)')\n",
    "    axs6[i,0].set_ylabel('Speech rate')\n",
    "    axs6[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'], label=\"Raw\", color=my_blue)\n",
    "    axs6[i,0].plot(adapted_data_1['Global_start'], adapted_data_1['Speech rate'],'o', label=\"Adapted\", color=my_red)\n",
    "    \n",
    "    axs6[i,1].set_title(filename+\" speaker2\")\n",
    "    axs6[i,1].set_xlabel('Time (sec)')\n",
    "    axs6[i,1].set_ylabel('Adapted speech rate')\n",
    "    axs6[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'],label=\"Raw\", color=my_blue)\n",
    "    axs6[i,1].plot(adapted_data_2['Global_start'], adapted_data_2['Speech rate'], 'o',label=\"Adapted\", color=my_red)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.close(fig6)\n",
    "fig6.savefig(plots_path+\"Adapted CA speech rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145896df",
   "metadata": {},
   "source": [
    "# Global correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "263adb22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m data_1 \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeaker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mspeaker1][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeech rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m data_2 \u001b[38;5;241m=\u001b[39m data[data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeaker\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39mspeaker2][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeech rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m correlation, p_value \u001b[38;5;241m=\u001b[39m \u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpearsonr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m correlation_list_AA\u001b[38;5;241m.\u001b[39mappend(correlation)\n\u001b[0;32m     13\u001b[0m p_values_AA\u001b[38;5;241m.\u001b[39mappend(p_value)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:4013\u001b[0m, in \u001b[0;36mpearsonr\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   4011\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[1;32m-> 4013\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx and y must have the same length.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   4015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   4016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx and y must have length at least 2.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have the same length."
     ]
    }
   ],
   "source": [
    "correlation_list_AA = []\n",
    "p_values_AA = []\n",
    "\n",
    "for data in adapted_data_AA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "    \n",
    "    data_1 = data[data['Speaker']==speaker1]['Speech rate']\n",
    "    data_2 = data[data['Speaker']==speaker2]['Speech rate']\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(np.array(data_1), np.array(data_2))\n",
    "    correlation_list_AA.append(correlation)\n",
    "    p_values_AA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93155f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig7 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "    \n",
    "    corr = correlation_list_AA[i]\n",
    "    p_value = p_values_AA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o', label=filename, color=my_colors[i])\n",
    "    plt.plot([\"p_value\"], [p_value], 'o',color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig7)\n",
    "fig7.savefig(plots_path+\"Global AA correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317dbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_CA = []\n",
    "p_values_CA = []\n",
    "\n",
    "for data in adapted_data_CA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "    \n",
    "    data_1 = data[data['Speaker']==speaker1]['Speech rate']\n",
    "    data_2 = data[data['Speaker']==speaker2]['Speech rate']\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(np.array(data_1), np.array(data_2))\n",
    "    correlation_list_CA.append(correlation)\n",
    "    p_values_CA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02191da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig8 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "    \n",
    "    corr = correlation_list_CA[i]\n",
    "    p_value = p_values_CA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o',color=my_colors[i], label=filename)\n",
    "    plt.plot([\"p_value\"], [p_value], 'o', color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig8)\n",
    "fig8.savefig(plots_path+\"Global CA correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84780e4",
   "metadata": {},
   "source": [
    "## Global correlation with default window smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_AA = []\n",
    "p_values_AA = []\n",
    "\n",
    "for data in adapted_data_AA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "\n",
    "    data_1 = data[data['Speaker']==speaker1]\n",
    "    data_2 = data[data['Speaker']==speaker2]\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(smooth_2(data_1['Speech rate'], default_window, default_poly), smooth_2(data_2['Speech rate'], default_window, default_poly))\n",
    "\n",
    "    correlation_list_AA.append(correlation)\n",
    "    p_values_AA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig9 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "    \n",
    "    corr = correlation_list_AA[i]\n",
    "    p_value = p_values_AA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o', label=filename, color=my_colors[i])\n",
    "    plt.plot([\"p_value\"], [p_value], 'o',color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig9)\n",
    "fig9.savefig(plots_path+\"Global AA smoothed correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97621c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_list_CA = []\n",
    "p_values_CA = []\n",
    "\n",
    "for data in adapted_data_CA :\n",
    "    \n",
    "    speaker1, speaker2 = np.unique(data['Speaker'])\n",
    "\n",
    "    data_1 = data[data['Speaker']==speaker1]\n",
    "    data_2 = data[data['Speaker']==speaker2]\n",
    "    \n",
    "    correlation, p_value = stats.pearsonr(smooth_2(data_1['Speech rate'], default_window, default_poly), smooth_2(data_2['Speech rate'], default_window, default_poly))\n",
    "\n",
    "    correlation_list_CA.append(correlation)\n",
    "    p_values_CA.append(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig10 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "    \n",
    "    corr = correlation_list_CA[i]\n",
    "    p_value = p_values_CA[i]\n",
    "    plt.plot([\"correlation\"], [corr], 'o', label=filename, color=my_colors[i])\n",
    "    plt.plot([\"p_value\"], [p_value], 'o',color=my_colors[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.close(fig10)\n",
    "fig10.savefig(plots_path+\"Global CA smoothed correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbaa7b4",
   "metadata": {},
   "source": [
    "# Influence of the smoothing window on global correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70597100",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = np.arange(5, 25, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_AA = []\n",
    "p_values_AA = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    correlation_file = []\n",
    "    p_value_file = []\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    data1  = data[data['Speaker']=='Adult1']\n",
    "    data2  = data[data['Speaker']=='Adult2']\n",
    "    \n",
    "    for window in windows :\n",
    "        \n",
    "        corr, p_value = stats.pearsonr(smooth_2(data1['Speech rate'], window, default_poly), smooth_2(data2['Speech rate'], window, default_poly))\n",
    "        correlation_file.append(corr)\n",
    "        p_value_file.append(p_value)\n",
    "        \n",
    "    correlations_AA.append(correlation_file)\n",
    "    p_values_AA.append(p_value_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e058cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig11 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "\n",
    "    plt.plot(windows, correlations_AA[i], label=filename, color=my_colors[i])\n",
    "    #plt.plot(windows, p_values_AA[i], '--',color=my_colors[i])\n",
    "\n",
    "plt.xlabel(\"smoothing window\")\n",
    "plt.ylabel(\"correlation\")\n",
    "plt.legend()\n",
    "plt.close(fig11)\n",
    "fig11.savefig(plots_path+\"Window influence on correlation AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52026186",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_CA = []\n",
    "p_values_CA = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    correlation_file = []\n",
    "    p_values_file = []\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    data1  = data[data['Speaker']=='Parent']\n",
    "    data2  = data[data['Speaker']=='Child']\n",
    "    \n",
    "    for window in windows :\n",
    "        \n",
    "        corr, p_value = stats.pearsonr(smooth_2(data1['Speech rate'], window, default_poly), smooth_2(data2['Speech rate'], window, default_poly))\n",
    "        correlation_file.append(corr)\n",
    "        p_values_file.append(p_value)\n",
    "        \n",
    "    correlations_CA.append(correlation_file)\n",
    "    p_values_CA.append(p_values_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig12 = plt.figure()\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "\n",
    "    plt.plot(windows, correlations_CA[i], label=filename, color=my_colors[i])\n",
    "    #plt.plot(windows, p_values_CA[i], \"--\", color=my_colors[i] )\n",
    "    \n",
    "plt.xlabel(\"smoothing window\")\n",
    "plt.ylabel(\"correlation\")\n",
    "plt.legend(loc=4)\n",
    "plt.close(fig12)\n",
    "fig12.savefig(plots_path+\"Window influence on correlation CA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019245ac",
   "metadata": {},
   "source": [
    "# Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1bd9d1",
   "metadata": {},
   "source": [
    "## The conversation contains three phases : pregame conversation (explanation of the rules, often a monologue), the guessing game, and postgame conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e5373",
   "metadata": {},
   "source": [
    "### Computing the correlation of each phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b69ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Phases = ['PREGAME', 'GAME', 'POSTGAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b6c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corr_files_AA = []\n",
    "nb_points_files_AA = []\n",
    "p_values_files_AA = []\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "\n",
    "    print(filename)\n",
    "    \n",
    "    list_corr = []\n",
    "        \n",
    "    df = pd.read_csv(path_AA+str(filename)+'.csv')\n",
    "    \n",
    "    corr_phases, nb_points, p_values = compute_corr_phases(df, Phases, \"Phases\", \"smoothed\", \"Speaker\", \"Global_start\")\n",
    "\n",
    "    p_values_files_AA.append(p_values)        \n",
    "    list_corr_files_AA.append(corr_phases)\n",
    "    nb_points_files_AA.append(nb_points)\n",
    "\n",
    "\n",
    "mean_corr = [my_mean([list_corr_files_AA[j][i] for i in range(len(list_corr_files_AA[j]))]) for j in range(len(list_corr_files_AA))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corr_files_CA = []\n",
    "nb_points_files_CA = []\n",
    "p_values_files_CA = []\n",
    "\n",
    "for i, filename in enumerate(filenames_CA):\n",
    "    list_corr = []\n",
    "        \n",
    "    file = data_CA[i]\n",
    "\n",
    "    df = pd.read_csv(path_CA+str(filename)+'.csv')\n",
    "    \n",
    "    corr_phases, nb_points, p_values = compute_corr_phases(df, Phases, \"Phases\", \"smoothed\", \"Speaker\", \"Global_start\")\n",
    "\n",
    "    p_values_files_CA.append(p_values)\n",
    "    list_corr_files_CA.append(corr_phases)\n",
    "    nb_points_files_CA.append(nb_points)\n",
    "\n",
    "mean_corr = [my_mean([list_corr_files_CA[j][i] for i in range(len(list_corr_files_CA[j]))]) for j in range(len(list_corr_files_CA))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eaea13",
   "metadata": {},
   "source": [
    "### Plotting phases on the speech rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acde0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig13, axs13 = plt.subplots(9, 1, figsize=(15,20))\n",
    "\n",
    "for i, table in enumerate(data_AA) :\n",
    "    \n",
    "    filename = filenames_AA[i]\n",
    "    \n",
    "    axs13[i].set_xlabel('time')\n",
    "    axs13[i].set_ylabel('Speech rate')\n",
    "    axs13[i].plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], default_window, default_poly), label='Adult1',color = my_red)\n",
    "    axs13[i].plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], default_window, default_poly), label='Adult2', color=my_blue)\n",
    "    axs13[i].set_title(str(table['Filename'][0]))\n",
    "    axs13[i].legend()\n",
    "    \n",
    "    liste_phases = list(phases_AA[phases_AA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    for couple in liste_phases:\n",
    "        \n",
    "        label, time = couple\n",
    "\n",
    "        if \"1\" in label:\n",
    "            axs13[i].axvline(x=time, color='r')\n",
    "        elif \"2\" in label:\n",
    "            axs13[i].axvline(x=time, color='b')\n",
    "        else :\n",
    "            axs13[i].axvline(x=time, color='orange')\n",
    "            \n",
    "    if \"AA-MG-CH\" in filename:\n",
    "        fig135 = plt.figure(figsize=(20,5))\n",
    "        plt.plot(table[table['Speaker']=='Adult1']['Global_start'], smooth_2(table[table['Speaker']=='Adult1']['Speech rate'], default_window, default_poly), label='Adult1',color = my_red)\n",
    "        plt.plot( table[table['Speaker']=='Adult2']['Global_start'], smooth_2(table[table['Speaker']=='Adult2']['Speech rate'], default_window, default_poly), label='Adult2', color=my_blue)\n",
    "        \n",
    "        for couple in liste_phases:\n",
    "        \n",
    "            label, time = couple\n",
    "\n",
    "            if \"1\" in label:\n",
    "                plt.axvline(x=time, color='r')\n",
    "            elif \"2\" in label:\n",
    "                plt.axvline(x=time, color='b')\n",
    "            else :\n",
    "                plt.axvline(x=time, color='orange')\n",
    "        \n",
    "        plt.close(fig135)\n",
    "        fig135.savefig(examples_path + \"Phases_SR_AA\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close(fig13)\n",
    "fig13.savefig(plots_path+\"Phases and smoothed speech rate AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec930cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig14, axs14 = plt.subplots(9, 1, figsize=(15,20))\n",
    "\n",
    "for i, table in enumerate(data_CA) :\n",
    "    \n",
    "    filename = filenames_CA[i]\n",
    "    \n",
    "    axs14[i].set_xlabel('time')\n",
    "    axs14[i].set_ylabel('Speech rate')\n",
    "    axs14[i].plot(table[table['Speaker']=='Parent']['Global_start'], smooth_2(table[table['Speaker']=='Parent']['Speech rate'], default_window, default_poly), label='Parent',color = my_red)\n",
    "    axs14[i].plot( table[table['Speaker']=='Child']['Global_start'], smooth_2(table[table['Speaker']=='Child']['Speech rate'], default_window, default_poly), label='Child', color=my_blue)\n",
    "    axs14[i].set_title(str(table['Filename'][0]))\n",
    "    axs14[i].legend()\n",
    "    \n",
    "    liste_phases = list(phases_CA[phases_CA['Filename']==filename]['Phases'])[0]\n",
    "    \n",
    "    for couple in liste_phases:\n",
    "        \n",
    "        label, time = couple\n",
    "\n",
    "        if \"1\" in label:\n",
    "            axs14[i].axvline(x=time, color='r')\n",
    "        elif \"2\" in label:\n",
    "            axs14[i].axvline(x=time, color='b')\n",
    "        else :\n",
    "            axs14[i].axvline(x=time, color='orange')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.close(fig14)\n",
    "fig14.savefig(plots_path+\"Phases and smoothed speech rate CA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2252173c",
   "metadata": {},
   "source": [
    "### Plotting correlation of each phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aba0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig15, axs15 = plt.subplots(3,1, figsize=(10,10))\n",
    "ax1, ax2, ax3 = axs15\n",
    "\n",
    "for i, list_corr in enumerate(list_corr_files_AA):\n",
    "    \n",
    "    ax1.plot(Phases, list_corr, 'o',label=str(filenames_AA[i]))\n",
    "    \n",
    "    ax2.plot(Phases, nb_points_files_AA[i],'o', label=str(filenames_AA[i]))\n",
    "    \n",
    "    ax3.plot(Phases, p_values_files_AA[i],'o', label=str(filenames_AA[i]))\n",
    "    \n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "\n",
    "ax1.set_ylabel(\"correlation\")\n",
    "ax2.set_ylabel(\"number of utterances points\")\n",
    "ax3.set_ylabel(\"p_value\")\n",
    "\n",
    "ax1.legend(loc=(0.2, 0.05))\n",
    "ax2.legend(loc=(0.2, 0.05))\n",
    "ax3.legend(loc=(0.2, 0.05))\n",
    "\n",
    "plt.close(fig15)\n",
    "fig15.savefig(plots_path+\"Correlation by phase AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86246934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig16, axs16 = plt.subplots(3,1, figsize=(10,10))\n",
    "ax1, ax2, ax3 = axs16\n",
    "\n",
    "for i, list_corr in enumerate(list_corr_files_CA):\n",
    "    \n",
    "    ax1.plot(Phases, list_corr, 'o',label=str(filenames_CA[i]))\n",
    "    \n",
    "    ax2.plot(Phases, nb_points_files_CA[i], 'o',label=str(filenames_CA[i]))\n",
    "    \n",
    "    ax3.plot(Phases, p_values_files_CA[i],'o', label=str(filenames_CA[i]))\n",
    "    \n",
    "ax1.yaxis.set_ticks_position('both')\n",
    "ax2.yaxis.set_ticks_position('both')\n",
    "\n",
    "ax1.set_ylabel(\"correlation\")\n",
    "ax2.set_ylabel(\"number of utterances points\")\n",
    "ax3.set_ylabel(\"p_value\")\n",
    "\n",
    "ax1.legend(loc=(0.2, 0.05))\n",
    "ax2.legend(loc=(0.2, 0.05))\n",
    "ax3.legend(loc=(0.2, 0.05))\n",
    "\n",
    "plt.close(fig16)\n",
    "fig16.savefig(plots_path+\"Correlation by phase CA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c69624",
   "metadata": {},
   "source": [
    "# Correlation by game role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e6358",
   "metadata": {},
   "source": [
    "## Each speaker is alternatively asking or answering the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roles_AA, Correlations_AA, p_values_AA = [],[], []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "\n",
    "    filename = filenames_AA[i]\n",
    "    \n",
    "    roles, correlations, p_values = compute_corr_roles(data, \"Roles\", \"Speech rate\", \"Speaker\", \"Global_start\")\n",
    "    \n",
    "    Roles_AA.append(roles)\n",
    "    Correlations_AA.append(correlations)\n",
    "    p_values_AA.append(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7852249",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig17, axs17 = plt.subplots(3,3, figsize=(10,10))\n",
    "\n",
    "\n",
    "for i in range(len(Roles_AA)):\n",
    "\n",
    "    axs17[i//3,i%3].plot(Roles_AA[i], Correlations_AA[i], 'o')\n",
    "    axs17[i//3,i%3].set_title(filenames_AA[i])\n",
    "\n",
    "plt.close(fig17)\n",
    "fig17.tight_layout()\n",
    "fig17.savefig(plots_path+\"Correlation by role AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roles_CA, Correlations_CA, p_values_CA = [],[], []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "\n",
    "    filename = filenames_CA[i]\n",
    "    \n",
    "    roles, correlations, p_values = compute_corr_roles(data, \"Roles\", \"Speech rate\", \"Speaker\", \"Global_start\")\n",
    "    \n",
    "    Roles_CA.append(roles)\n",
    "    Correlations_CA.append(correlations)\n",
    "    p_values_CA.append(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig18, axs18 = plt.subplots(3,3, figsize=(10,10))\n",
    "\n",
    "\n",
    "for i in range(len(Roles_CA)):\n",
    "\n",
    "    axs18[i//3,i%3].plot(Roles_CA[i], Correlations_CA[i], 'o')\n",
    "    axs18[i//3,i%3].set_title(filenames_CA[i])\n",
    "    \n",
    "plt.close(fig18)\n",
    "fig18.tight_layout()\n",
    "fig18.savefig(plots_path+\"Correlation by role CA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6b1b5",
   "metadata": {},
   "source": [
    "# Number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00165c1a",
   "metadata": {},
   "source": [
    "## Adult/adult vs Adult/child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f451fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words_adult_1 = []\n",
    "nb_words_adult_2 = []\n",
    "\n",
    "\n",
    "for i, data in enumerate(data_AA) :\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Adult1\"]\n",
    "    data_2 = data[data[\"Speaker\"]==\"Adult2\"]\n",
    "    \n",
    "    mean_1 = np.mean(data_1[\"Word\"])\n",
    "    mean_2 = np.mean(data_2[\"Word\"])\n",
    "    \n",
    "    nb_words_adult_1.append(mean_1)\n",
    "    nb_words_adult_2.append(mean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words_adult = []\n",
    "nb_words_child = []\n",
    "\n",
    "\n",
    "for i, data in enumerate(data_CA) :\n",
    "    \n",
    "    \n",
    "    data_adult = data[data[\"Speaker\"]==\"Parent\"]\n",
    "    data_child = data[data[\"Speaker\"]==\"Child\"]\n",
    "        \n",
    "    mean_adult = np.mean(data_adult[\"Word\"])\n",
    "    mean_child = np.mean(data_child[\"Word\"])\n",
    "    \n",
    "    nb_words_adult.append(mean_adult)\n",
    "    nb_words_child.append(mean_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8051b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig19, axs19 = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "for i, filename in enumerate(filenames_AA):\n",
    "    \n",
    "    axs19[0].plot([\"Adult 1\", \"Adult 2\"], [nb_words_adult_1[i], nb_words_adult_2[i]], 'o', label=filename)\n",
    "    \n",
    "axs19[0].axhline(y=np.mean(nb_words_adult_1), label=\"Adult1\", color='r')\n",
    "axs19[0].axhline(y=np.mean(nb_words_adult_2), label=\"Adult2\", color='b')\n",
    "\n",
    "for j, filename in enumerate(filenames_CA):\n",
    "    \n",
    "    axs19[1].plot([\"Parent\", \"Child\"], [nb_words_adult[j], nb_words_child[j]], 'o', label=filename)\n",
    "    \n",
    "axs19[1].axhline(y=np.mean(nb_words_adult), label=\"Adult\", color='r')\n",
    "axs19[1].axhline(y=np.mean(nb_words_child), label=\"Child\", color='b')\n",
    "    \n",
    "axs19[0].set_title(\"Mean number of words for both adults\")\n",
    "axs19[1].set_title(\"Mean number of words for adult and child\")\n",
    "\n",
    "axs19[0].legend(loc=9,fontsize=8)\n",
    "axs19[1].legend(loc=9,fontsize=8)\n",
    "\n",
    "plt.close(fig19)\n",
    "fig19.savefig(plots_path+\"Mean number of words AA VS CA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae6d25",
   "metadata": {},
   "source": [
    "## Coding roles in a more convenient way (ask or answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    roles_file = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        if (data[\"Speaker\"][j]==\"Adult1\" and data[\"Roles\"][j]==\"SPEAKER1 \") or (data[\"Speaker\"][j]==\"Adult2\" and data[\"Roles\"][j]==\"SPEAKER2 \"):\n",
    "            roles_file.append(\"ans\")\n",
    "            \n",
    "        else:\n",
    "            roles_file.append(\"ask\")\n",
    "            \n",
    "    data[\"New_role\"]=roles_file\n",
    "    adapted_data_AA[i] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    roles_file = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        if (data[\"Speaker\"][j]==\"Parent\" and data[\"Roles\"][j]==\"SPEAKER1 \") or (data[\"Speaker\"][j]==\"Child\" and data[\"Roles\"][j]==\"SPEAKER2 \"):\n",
    "            roles_file.append(\"ans\")\n",
    "            \n",
    "        else:\n",
    "            roles_file.append(\"ask\")\n",
    "            \n",
    "    data[\"New_role\"]=roles_file\n",
    "    adapted_data_CA[i] = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdcebb8",
   "metadata": {},
   "source": [
    "## Number of words by role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_1_ans = []\n",
    "adult_1_ask = []\n",
    "\n",
    "adult_2_ans = []\n",
    "adult_2_ask = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Adult1\"]\n",
    "    \n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Word\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Word\"])\n",
    "    \n",
    "    adult_1_ans.append(mean_1_ans)\n",
    "    adult_1_ask.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Adult2\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Word\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Word\"])\n",
    "    \n",
    "    adult_2_ans.append(mean_2_ans)\n",
    "    adult_2_ask.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_ans = []\n",
    "adult_ask = []\n",
    "\n",
    "child_ans = []\n",
    "child_ask = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Parent\"]\n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Word\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Word\"])\n",
    "    \n",
    "    adult_ans.append(mean_1_ans)\n",
    "    adult_ask.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Child\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Word\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Word\"])\n",
    "    \n",
    "    child_ans.append(mean_2_ans)\n",
    "    child_ask.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = [\"Adult 1\" for i in range(len(adult_1_ans)+len(adult_1_ask))]+[\"Adult 2\" for i in range(len(adult_2_ans)+len(adult_2_ask))]\n",
    "role = [\"Answer\" for i in range(len(adult_1_ans))] + [\"Ask\" for i in range(len(adult_1_ask))] + [\"Answer\" for i in range(len(adult_2_ans))]+ [\"Ask\" for i in range(len(adult_2_ask))]\n",
    "words = adult_1_ans + adult_1_ask + adult_2_ans + adult_2_ask\n",
    "dico_AA = {\"Speaker\":speaker, \"Role\":role,\"Words\":words}\n",
    "df_AA = pd.DataFrame(dico_AA)\n",
    "\n",
    "fig20 = sns.catplot(data=df_AA, kind=\"bar\", x=\"Role\", y=\"Words\", hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig20.despine(left=True)\n",
    "fig20.set_axis_labels(\"\", \"Mean number of words per utterance\")\n",
    "fig20.legend.set_title(\"\")\n",
    "\n",
    "fig20.savefig(plots_path+\"Mean nb of words by role AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = [\"Adult\" for i in range(len(adult_ans)+len(adult_ask))]+[\"Child\" for i in range(len(child_ans)+len(child_ask))]\n",
    "role = [\"Answer\" for i in range(len(adult_ans))] + [\"Ask\" for i in range(len(adult_ask))] + [\"Answer\" for i in range(len(child_ans))]+ [\"Ask\" for i in range(len(child_ask))]\n",
    "words = adult_ans + adult_ask + child_ans + child_ask\n",
    "dico_CA = {\"Speaker\":speaker, \"Role\":role,\"Words\":words}\n",
    "df_CA = pd.DataFrame(dico_CA)\n",
    "\n",
    "fig21 = sns.catplot(data=df_CA, kind=\"bar\", x=\"Role\", y=\"Words\",hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig21.despine(left=True)\n",
    "fig21.set_axis_labels(\"\", \"Mean number of words per utterance\")\n",
    "fig21.legend.set_title(\"\")\n",
    "\n",
    "fig21.savefig(plots_path+\"Mean nb of words by role CA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c1396",
   "metadata": {},
   "source": [
    "# Mean speech rate by role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31250483",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_1_ans_SR = []\n",
    "adult_1_ask_SR = []\n",
    "\n",
    "adult_2_ans_SR = []\n",
    "adult_2_ask_SR = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_AA):\n",
    "    \n",
    "    filename = adapted_filenames_AA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Adult1\"]\n",
    "    \n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Speech rate\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Speech rate\"])\n",
    "    \n",
    "    adult_1_ans_SR.append(mean_1_ans)\n",
    "    adult_1_ask_SR.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Adult2\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Speech rate\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Speech rate\"])\n",
    "    \n",
    "    adult_2_ans_SR.append(mean_2_ans)\n",
    "    adult_2_ask_SR.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b203a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_ans_SR = []\n",
    "adult_ask_SR = []\n",
    "\n",
    "child_ans_SR = []\n",
    "child_ask_SR = []\n",
    "\n",
    "for i, data in enumerate(adapted_data_CA):\n",
    "    \n",
    "    filename = adapted_filenames_CA[i]\n",
    "    \n",
    "    data_1 = data[data[\"Speaker\"]==\"Parent\"]\n",
    "    data_1_ans = data_1[data_1[\"New_role\"]==\"ans\"]\n",
    "    data_1_ask = data_1[data_1[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_1_ans = np.mean(data_1_ans[\"Speech rate\"])\n",
    "    mean_1_ask = np.mean(data_1_ask[\"Speech rate\"])\n",
    "    \n",
    "    adult_ans_SR.append(mean_1_ans)\n",
    "    adult_ask_SR.append(mean_1_ask)\n",
    "    \n",
    "    data_2 = data[data[\"Speaker\"]==\"Child\"]\n",
    "    data_2_ans = data_2[data_2[\"New_role\"]==\"ans\"]\n",
    "    data_2_ask = data_2[data_2[\"New_role\"]==\"ask\"]\n",
    "    \n",
    "    mean_2_ans = np.mean(data_2_ans[\"Speech rate\"])\n",
    "    mean_2_ask = np.mean(data_2_ask[\"Speech rate\"])\n",
    "    \n",
    "    child_ans_SR.append(mean_2_ans)\n",
    "    child_ask_SR.append(mean_2_ask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57150a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = [\"Adult 1\" for i in range(len(adult_1_ans_SR)+len(adult_1_ask_SR))]+[\"Adult 2\" for i in range(len(adult_2_ans_SR)+len(adult_2_ask_SR))]\n",
    "role = [\"Answer\" for i in range(len(adult_1_ans_SR))] + [\"Ask\" for i in range(len(adult_1_ask_SR))] + [\"Answer\" for i in range(len(adult_2_ans_SR))]+ [\"Ask\" for i in range(len(adult_2_ask_SR))]\n",
    "SR = adult_1_ans_SR + adult_1_ask_SR + adult_2_ans_SR + adult_2_ask_SR\n",
    "dico_AA = {\"Speaker\":speaker, \"Role\":role,\"SR\":SR}\n",
    "df_AA = pd.DataFrame(dico_AA)\n",
    "\n",
    "fig22 = sns.catplot(data=df_AA, kind=\"bar\", x=\"Role\", y=\"SR\", hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig22.despine(left=True)\n",
    "fig22.set_axis_labels(\"\", \"Mean speech rate\")\n",
    "fig22.legend.set_title(\"\")\n",
    "\n",
    "fig22.savefig(plots_path+\"Mean SR by role AA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = [\"Adult\" for i in range(len(adult_ans_SR)+len(adult_ask_SR))]+[\"Child\" for i in range(len(child_ans_SR)+len(child_ask_SR))]\n",
    "role = [\"Answer\" for i in range(len(adult_ans_SR))] + [\"Ask\" for i in range(len(adult_ask_SR))] + [\"Answer\" for i in range(len(child_ans_SR))]+ [\"Ask\" for i in range(len(child_ask_SR))]\n",
    "SR = adult_ans_SR + adult_ask_SR + child_ans_SR + child_ask_SR\n",
    "dico_CA = {\"Speaker\":speaker, \"Role\":role,\"SR\":SR}\n",
    "df_CA = pd.DataFrame(dico_CA)\n",
    "\n",
    "fig23 = sns.catplot(data=df_CA, kind=\"bar\", x=\"Role\", y=\"SR\",hue=\"Speaker\", ci=\"sd\", palette=\"dark\", alpha=.6, height=6)\n",
    "fig23.despine(left=True)\n",
    "fig23.set_axis_labels(\"\", \"Mean SR per utterance\")\n",
    "fig23.legend.set_title(\"\")\n",
    "\n",
    "fig23.savefig(plots_path+\"Mean SR by role CA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OK !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183f933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
